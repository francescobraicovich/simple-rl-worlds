environment:
  name: "ALE/Assault-v5" # Simple environment for testing (was "ALE/Assault-v5")

  # Image preprocessing parameters
  frame_stack_size: 4         # Number of frames to stack (1 = no stacking, 4 = stack 4 frames)

data_and_patching:
  image_height: 84
  image_width: 84
  sequence_length: 4

# --- Data Collection & Dataset Management ---
data_collection:
  num_episodes: 3
  max_steps_per_episode: 100
  random_action_percentage: 0.2 # Percentage of actions to be random (0.0 = no random actions)
  load_path: "assault_rep_8.pth"
  filename: "assault_rep_8.pth"
  validation_split: 0.15
  ppo_agent:
    load: False # Whether to load a pre-trained PPO agent
    n_envs: 10 # Number of parallel environments for PPO training (default: CPU count)
    action_repetition_k: 8
    learning_rate: 0.0003
    total_train_timesteps: 500 # Timesteps to train PPO before data collection (reduced for testing)
    n_steps: 128       # PPO n_steps parameter
    batch_size: 256      # PPO batch_size parameter
    n_epochs: 6         # PPO n_epochs parameter
    gamma: 0.99         # PPO gamma parameter
    gae_lambda: 0.95    # PPO gae_lambda parameter
    clip_range: 0.2     # PPO clip_range parameter (reduced from 0.3 for better exploration)
    ent_coef: 0.015      # Entropy coefficient for exploration (encourages action diversity)
    additional_log_std_noise: 0.0
    policy_type: "CnnPolicy" # Policy type, e.g., "CnnPolicy" for image-based envs

latent_dim: 128 # Dimension of the embedding space for the transformer model

training:
  main_loops:
    num_epochs: 50 # Number of epochs for training
    batch_size: 128 # Batch size for training
    learning_rate: 0.0001 # Learning rate for the optimizer
    weight_decay: 0.0001 # Weight decay for the optimizer
    ema_decay: 0.999 # Exponential moving average decay for model parameters
    gradient_clipping: 1.0 # Maximum norm for gradient clipping
    reward_loss: true # Whether to use reward loss during training
    # Learning rate scheduler configuration
    lr_scheduler:
      enabled: true # Whether to use learning rate scheduling
      type: "cosine" # Type of scheduler: "cosine", "step", "exponential", "plateau"
      # Cosine annealing parameters
      cosine_T_max: 50 # Maximum number of iterations for cosine annealing (typically num_epochs)
      cosine_eta_min: 0.000001 # Minimum learning rate for cosine annealing
      # Step scheduler parameters  
      step_size: 20 # Period of learning rate decay for step scheduler
      step_gamma: 0.5 # Multiplicative factor of learning rate decay for step scheduler
      # Exponential scheduler parameters
      exp_gamma: 0.95 # Multiplicative factor for exponential decay
      # ReduceLROnPlateau parameters
      plateau_mode: "min" # Mode for plateau scheduler: "min" or "max"
      plateau_factor: 0.5 # Factor by which learning rate is reduced
      plateau_patience: 10 # Number of epochs with no improvement to wait
      plateau_threshold: 0.0001 # Threshold for measuring improvement
  jepa_decoder:
    num_epochs: 50 # Number of epochs for the JEPA decoder training
    batch_size: 128 # Batch size for the JEPA decoder training
    learning_rate: 0.0001 # Learning rate for the JEPA decoder optimizer
    weight_decay: 0.0001 # Weight decay for the JEPA decoder optimizer
    gradient_clipping: 1.0 # Maximum norm for JEPA decoder gradient clipping
    # Learning rate scheduler configuration
    lr_scheduler:
      enabled: true # Whether to use learning rate scheduling
      type: "cosine" # Type of scheduler: "cosine", "step", "exponential", "plateau"
      cosine_T_max: 50 # Maximum number of iterations for cosine annealing
      cosine_eta_min: 0.000001 # Minimum learning rate for cosine annealing
  reward_predictor:
    versions: "both" # Versions of the reward predictor to train (e.g., "both", "jepa", "encoder_decoder")
    num_epochs: 100 # Number of epochs for the reward predictor training
    batch_size: 128 # Batch size for the reward predictor training
    learning_rate: 0.0001 # Learning rate for the reward predictor optimizer
    weight_decay: 0.0001 # Weight decay for the reward predictor optimizer
    gradient_clipping: 0.5 # Maximum norm for reward predictor gradient clipping
    # Learning rate scheduler configuration
    lr_scheduler:
      enabled: true # Whether to use learning rate scheduling
      type: "cosine" # Type of scheduler: "cosine", "step", "exponential", "plateau"
      cosine_T_max: 100 # Maximum number of iterations for cosine annealing
      cosine_eta_min: 0.000001 # Minimum learning rate for cosine annealing
  dynamics_reward_predictor:
    versions: "both" # Versions of the dynamics reward predictor to train (e.g., "both", "jepa", "encoder_decoder")
    num_epochs: 50 # Number of epochs for the dynamics reward predictor training
    batch_size: 128 # Batch size for the dynamics reward predictor training
    learning_rate: 0.0001 # Learning rate for the dynamics reward predictor optimizer
    weight_decay: 0.0001 # Weight decay for the dynamics reward predictor optimizer
    gradient_clipping: 0.5 # Maximum norm for gradient clipping
    # Learning rate scheduler configuration
    lr_scheduler:
      enabled: false # Whether to use learning rate scheduling
      type: "cosine" # Type of scheduler: "cosine", "step", "exponential", "plateau"
      cosine_T_max: 50 # Maximum number of iterations for cosine annealing
      cosine_eta_min: 0.000001 # Minimum learning rate for cosine annealing

models:
  encoder:
    conv_channels: [32, 64, 128, 256]
    activation: "silu"
    dropout_rate: 0
   
  predictor:
    hidden_sizes: [512, 512]
    activation: "silu"
    dropout_rate: 0.1
    
  decoder:
    initial_size: 4
    conv_channels: [256, 128, 64, 32, 1]
    activation: "silu"
    dropout_rate: 0

  reward_predictor:
    hidden_sizes: [256, 256]
    dropout: 0.1
    
  vicreg:
    active: false  # Whether to use VICReg loss for JEPA
    lambda_v: 25.0  # Weight for variance loss
    lambda_s: 1.0  # Weight for covariance loss
    lambda_i: 5.0   # Weight for invariance loss
    proj_hidden_dim: 256 # Hidden dimension of the VICReg projection head
    proj_output_dim: 32  # Output dimension of the VICReg projection head


# Wandb configuration for experiment tracking
wandb:
  enabled: true  # Set to true to enable wandb logging
  project: "simple-rl-worlds"
  entity: null  # Set to your wandb username/team
  tags: []  # Add tags for organizing experiments


    