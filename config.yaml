environment:
  name: "ALE/Breakout-v5" # Simple environment for testing (was "ALE/Assault-v5")

  # Image preprocessing parameters
  frame_stack_size: 4         # Number of frames to stack (1 = no stacking, 4 = stack 4 frames)

data_and_patching:
  image_height: 84
  image_width: 64
  sequence_length: 6
  patch_size_h: 8              # Patch size for image preprocessing
  patch_size_w: 8              # Patch size for image preprocessing
  patch_size_t: 2              # Temporal patch size for video data

# --- Data Collection & Dataset Management ---
data_collection:
  num_episodes: 10
  max_steps_per_episode: 256
  random_action_percentage: 0.15 # Percentage of actions to be random (0.0 = no random actions)
  load_path: "breakout_rep_4.pth"
  filename: "breakout_rep_4.pth"
  validation_split: 0.15
  ppo_agent:
    load: false # Whether to load a pre-trained PPO agent
    n_envs: 10 # Number of parallel environments for PPO training (default: CPU count)
    action_repetition_k: 4
    learning_rate: 0.0003
    total_train_timesteps: 1000000 # Timesteps to train PPO before data collection (reduced for testing)
    n_steps: 128       # PPO n_steps parameter
    batch_size: 128      # PPO batch_size parameter
    n_epochs: 6         # PPO n_epochs parameter
    gamma: 0.99         # PPO gamma parameter
    gae_lambda: 0.95    # PPO gae_lambda parameter
    clip_range: 0.2     # PPO clip_range parameter (reduced from 0.3 for better exploration)
    ent_coef: 0.01      # Entropy coefficient for exploration (encourages action diversity)
    additional_log_std_noise: 0.0
    policy_type: "CnnPolicy" # Policy type, e.g., "CnnPolicy" for image-based envs

embed_dim: 64 # Dimension of the embedding space for the transformer model

training:
  main_loops:
    num_epochs: 30 # Number of epochs for training
    batch_size: 64 # Batch size for training
    learning_rate: 0.0001 # Learning rate for the optimizer
    weight_decay: 0.0001 # Weight decay for the optimizer
    ema_decay: 0.999 # Exponential moving average decay for model parameters
    gradient_clipping: 0.5 # Maximum norm for gradient clipping
  jepa_decoder:
    num_epochs: 30 # Number of epochs for the JEPA decoder training
    batch_size: 256 # Batch size for the JEPA decoder training
    learning_rate: 0.0001 # Learning rate for the JEPA decoder optimizer
    weight_decay: 0.0001 # Weight decay for the JEPA decoder optimizer
    gradient_clipping: 0.5 # Maximum norm for JEPA decoder gradient clipping
  reward_predictor:
    versions: "both" # Versions of the reward predictor to train (e.g., "both", "jepa", "encoder_decoder")
    num_epochs: 40 # Number of epochs for the reward predictor training
    batch_size: 256 # Batch size for the reward predictor training
    learning_rate: 0.0001 # Learning rate for the reward predictor optimizer
    weight_decay: 0.0001 # Weight decay for the reward predictor optimizer
    gradient_clipping: 0.5 # Maximum norm for reward predictor gradient clipping
  dynamics_reward_predictor:
    versions: "both" # Versions of the dynamics reward predictor to train (e.g., "both", "jepa", "encoder_decoder")
    num_epochs: 40 # Number of epochs for the dynamics reward predictor training
    batch_size: 256 # Batch size for the dynamics reward predictor training
    learning_rate: 0.0001 # Learning rate for the dynamics reward predictor optimizer
    weight_decay: 0.0001 # Weight decay for the dynamics reward predictor optimizer
    gradient_clipping: 0.5 # Maximum norm for gradient clipping

models:
  encoder:
    num_layers: 8
    num_heads: 4
    mlp_ratio: 4.0
    dropout: 0.1
    attention_dropout: 0.1
    predictor_drop_path_rate: 0.1
  predictor:
    num_actions: 6
    num_layers: 4
    num_heads: 2
    mlp_ratio: 4.0
    dropout: 0.1
    attention_dropout: 0.1
    predictor_drop_path_rate: 0.1
  decoder:
    num_heads: 8
    dropout: 0.1
    attention_dropout: 0.1
    decoder_drop_path_rate: 0.1
  reward_predictor:
    internal_embedding_dim: 128
    num_heads: 8
    num_attention_layers: 3
    mlp_hidden_layers: [256, 128, 64]
    dropout: 0.1
    attention_dropout: 0.1
    use_layer_norm: true
    activation: "gelu"


# Wandb configuration for experiment tracking
wandb:
  enabled: true  # Set to true to enable wandb logging
  project: "simple-rl-worlds"
  entity: null  # Set to your wandb username/team
  tags: []  # Add tags for organizing experiments


    