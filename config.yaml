environment:
  name: "ALE/Assault-v5" # Simple environment for testing (was "ALE/Assault-v5")
  
  # Image preprocessing parameters
  frame_stack_size: 4         # Number of frames to stack (1 = no stacking, 4 = stack 4 frames)

data_and_patching:
  image_height: 64
  image_width: 64
  sequence_length: 6
  patch_size_h: 8              # Patch size for image preprocessing
  patch_size_w: 8              # Patch size for image preprocessing
  patch_size_t: 2              # Temporal patch size for video data

# --- Data Collection & Dataset Management ---
data_collection:
  num_episodes: 5
  max_steps_per_episode: 20
  random_action_percentage: 0.0 # Percentage of actions to be random (0.0 = no random actions)
  load_path: "assault_rep_4.pth"
  filename: "assault_rep_4.pth"
  validation_split: 0.2
  ppo_agent:
    load: false # Whether to load a pre-trained PPO agent
    n_envs: 10 # Number of parallel environments for PPO training (default: CPU count)
    action_repetition_k: 4
    learning_rate: 0.0003
    total_train_timesteps: 100 # Timesteps to train PPO before data collection (reduced for testing)
    n_steps: 32       # PPO n_steps parameter
    batch_size: 256      # PPO batch_size parameter
    n_epochs: 5         # PPO n_epochs parameter
    gamma: 0.99         # PPO gamma parameter
    gae_lambda: 0.95    # PPO gae_lambda parameter
    clip_range: 0.3     # PPO clip_range parameter
    additional_log_std_noise: 0.0
    policy_type: "CnnPolicy" # Policy type, e.g., "CnnPolicy" for image-based envs

embed_dim: 12 # Dimension of the embedding space for the transformer model

training:
  main_loops:
    num_epochs: 10 # Number of epochs for training
    batch_size: 32 # Batch size for training
    learning_rate: 0.0001 # Learning rate for the optimizer
    weight_decay: 0.0001 # Weight decay for the optimizer
    ema_decay: 0.999 # Exponential moving average decay for model parameters
  jepa_decoder:
    num_epochs: 10 # Number of epochs for the JEPA decoder training
    batch_size: 32 # Batch size for the JEPA decoder training
    learning_rate: 0.0001 # Learning rate for the JEPA decoder optimizer
    weight_decay: 0.0001 # Weight decay for the JEPA decoder optimizer

models:
  encoder:
    num_layers: 6
    num_heads: 2
    mlp_ratio: 4.0
    dropout: 0.1
    attention_dropout: 0.1
    predictor_drop_path_rate: 0.1
  predictor:
    num_actions: 18
    num_layers: 4
    num_heads: 2
    mlp_ratio: 4.0
    dropout: 0.1
    attention_dropout: 0.1
    predictor_drop_path_rate: 0.1
  decoder:
    num_layers: 6
    num_heads: 8
    mlp_ratio: 4.0
    dropout: 0.1
    attention_dropout: 0.1
    decoder_drop_path_rate: 0.1
    num_upsampling_blocks: 2 # Number of upsampling blocks in the decoder



    