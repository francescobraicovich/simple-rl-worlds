environment:
  name: "ALE/Assault-v5" # Simple environment for testing (was "ALE/Assault-v5")

  # Image preprocessing parameters
  frame_stack_size: 4         # Number of frames to stack (1 = no stacking, 4 = stack 4 frames)

data_and_patching:
  image_height: 224            # Updated for standard ResNet input size
  image_width: 224             # Updated for standard ResNet input size
  sequence_length: 4
  patch_size_h: 16             # Patch size for image preprocessing (adjusted for 224x224)
  patch_size_w: 16             # Patch size for image preprocessing (adjusted for 224x224)
  patch_size_t: 2              # Temporal patch size for video data

# --- Data Collection & Dataset Management ---
data_collection:
  num_episodes: 15               # ↓ Reduced from 30 for faster data loading
  max_steps_per_episode: 1500    # ↓ Reduced from 3000 for faster data loading
  random_action_percentage: 0.15 # Percentage of actions to be random (0.0 = no random actions)
  load_path: "assault_rep_8.pth"
  filename: "assault_rep_8.pth"
  validation_split: 0.2
  ppo_agent:
    load: False # Whether to load a pre-trained PPO agent
    n_envs: 10 # Number of parallel environments for PPO training (default: CPU count)
    action_repetition_k: 8
    learning_rate: 0.0003
    total_train_timesteps: 100000 # Timesteps to train PPO before data collection (very small for testing)
    n_steps: 128       # PPO n_steps parameter
    batch_size: 256      # PPO batch_size parameter
    n_epochs: 6         # PPO n_epochs parameter
    gamma: 0.99         # PPO gamma parameter
    gae_lambda: 0.95    # PPO gae_lambda parameter
    clip_range: 0.2     # PPO clip_range parameter (reduced from 0.3 for better exploration)
    ent_coef: 0.015      # Entropy coefficient for exploration (encourages action diversity)
    additional_log_std_noise: 0.0
    policy_type: "CnnPolicy" # Policy type, e.g., "CnnPolicy" for image-based envs

latent_dim: 128 # Reduced to match MobileNetV2 encoder parameter count (~3.5M)

training:
  main_loops:
    num_epochs: 50              
    batch_size: 32               
    learning_rate: 0.001        
    weight_decay: 0.0001         
    ema_decay: 0.999             
    gradient_clipping: 1.0       
    
    early_stopping:
      enabled: false
      patience: 2               # ↑ Increased patience for fewer epochs
      min_delta: 0.001           # ↑ Increased threshold for faster stopping
      restore_best_weights: true
    
    lr_scheduler:
      enabled: true
      type: "cosine"             # ✅ Best for most cases
      cosine_T_max: 50           # Match num_epochs
      cosine_eta_min: 0.0001     # ↑ Higher minimum LR for faster convergence
  jepa_decoder:
    num_epochs: 50 # Number of epochs for the JEPA decoder training
    batch_size: 128 # Batch size for the JEPA decoder training
    learning_rate: 0.0001 # Learning rate for the JEPA decoder optimizer
    weight_decay: 0.0001 # Weight decay for the JEPA decoder optimizer
    gradient_clipping: 1.0 # Maximum norm for JEPA decoder gradient clipping
    # Learning rate scheduler configuration
    lr_scheduler:
      enabled: true # Whether to use learning rate scheduling
      type: "cosine" # Type of scheduler: "cosine", "step", "exponential", "plateau"
      cosine_T_max: 50 # Maximum number of iterations for cosine annealing
      cosine_eta_min: 0.000001 # Minimum learning rate for cosine annealing
  reward_predictor:
    versions: "both" # Versions of the reward predictor to train (e.g., "both", "jepa", "encoder_decoder")
    num_epochs: 50                 # ↓ Reduced from 200 for faster training
    batch_size: 64                 # ↑ Larger batches
    learning_rate: 0.001           # ↑ Increased from 0.0003 for faster convergence
    weight_decay: 0.0001 # Weight decay for the reward predictor optimizer
    gradient_clipping: 0.5 # Maximum norm for reward predictor gradient clipping
    # Early stopping configuration
    early_stopping:
      enabled: true # Whether to use early stopping
      patience: 15                 # ↓ Reduced patience for faster stopping
      min_delta: 0.001             # ↑ Increased threshold for faster stopping
      restore_best_weights: true # Whether to restore best weights when stopping early
    # Learning rate scheduler configuration
    lr_scheduler:
      enabled: true # Whether to use learning rate scheduling
      type: "cosine" # Type of scheduler: "cosine", "step", "exponential", "plateau"
      cosine_T_max: 50 # Maximum number of iterations for cosine annealing
      cosine_eta_min: 0.0001 # Minimum learning rate for cosine annealing
  dynamics_reward_predictor:
    versions: "both" # Versions of the dynamics reward predictor to train (e.g., "both", "jepa", "encoder_decoder")
    num_epochs: 50 # Number of epochs for the dynamics reward predictor training
    batch_size: 128 # Batch size for the dynamics reward predictor training
    learning_rate: 0.0001 # Learning rate for the dynamics reward predictor optimizer
    weight_decay: 0.0001 # Weight decay for the dynamics reward predictor optimizer
    gradient_clipping: 0.5 # Maximum norm for gradient clipping
    # Learning rate scheduler configuration
    lr_scheduler:
      enabled: false # Whether to use learning rate scheduling
      type: "cosine" # Type of scheduler: "cosine", "step", "exponential", "plateau"
      cosine_T_max: 50 # Maximum number of iterations for cosine annealing
      cosine_eta_min: 0.000001 # Minimum learning rate for cosine annealing

models:
  encoder:
    use_pretrained_backbone: true    # Use pretrained backbone for better features
    backbone_type: "mobilenetv2"        # Options: "resnet18", "mobilenetv2"
    input_channels: 3                # RGB input (changed from 1 for grayscale)
    image_size: 224                  # Standard backbone input size
    conv_channels: [32, 64, 128, 256]  # Only used if use_pretrained_backbone: false
    activation: "silu"
    dropout_rate: 0.2              # ↑ More regularization for pretrained models
   
  predictor:
    hidden_sizes: [1024, 1024]   # ↑ Larger for complex dynamics
    activation: "silu"
    dropout_rate: 0.2            # ↑ More regularization
    
  decoder:
    initial_size: 3              # Optimized to match MobileNetV2 encoder parameter count (~3.1M)
    conv_channels: [512, 256, 64, 32, 3]  # Optimized for ~3.1M parameters (4.4% difference)
    activation: "silu"
    dropout_rate: 0.2            # ↑ More regularization

  reward_predictor:
    hidden_sizes: [256, 256]
    dropout: 0.1
    
  vicreg:
    active: false  # Whether to use VICReg loss for JEPA
    lambda_v: 25.0  # Weight for variance loss
    lambda_s: 1.0  # Weight for covariance loss
    lambda_i: 5.0   # Weight for invariance loss
    proj_hidden_dim: 256 # Hidden dimension of the VICReg projection head
    proj_output_dim: 32  # Output dimension of the VICReg projection head


# Wandb configuration for experiment tracking
wandb:
  enabled: true  # Set to true to enable wandb logging
  project: "simple-rl-worlds"
  entity: null  # Set to your wandb username/team
  tags: ["mobilenetv2", "rgb", "224x224"]  # Add tags for organizing experiments


    