Running on node: gnode02
Using 4 CPU cores
CUDA devices available:
Sun Jul 13 19:13:10 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100 80GB PCIe           On | 00000000:65:00.0 Off |                    0 |
| N/A   42C    P0               45W / 300W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+

Running stage: model_init
ğŸš€ MODEL INITIALIZATION AND PARAMETER ANALYSIS
================================================================================
Using device: cuda

ğŸ“Š Initializing models...

1. Initializing Encoder...
2. Initializing Predictor...
3. Initializing Decoder...
4. Initializing Reward Predictor...

âœ… All models initialized successfully!

============================================================
ENCODER MODEL INFORMATION
============================================================
Model Class: VideoViT
Total Parameters: 1.20M (1,195,904)
Trainable Parameters: 1.20M (1,195,904)
Non-trainable Parameters: 0 (0)

Model Architecture Summary:
  Device: cuda:0
  Dtype: torch.float32
  Layer Types:
    DropPath: 12
    Dropout: 18
    GELU: 6
    LayerNorm: 13
    Linear: 24
    MLP: 6
    MultiHeadSelfAttention: 6
    RotaryEmbedding: 6
    TransformerBlock: 6

============================================================
PREDICTOR MODEL INFORMATION
============================================================
Model Class: LatentDynamicsPredictor
Total Parameters: 810.62K (810,624)
Trainable Parameters: 810.62K (810,624)
Non-trainable Parameters: 0 (0)

Model Architecture Summary:
  Device: cuda:0
  Dtype: torch.float32
  Layer Types:
    DropPath: 8
    Dropout: 12
    GELU: 4
    LayerNorm: 9
    Linear: 17
    MLP: 4
    MultiHeadSelfAttention: 4
    RotaryEmbedding: 4
    TransformerBlock: 4

============================================================
DECODER MODEL INFORMATION
============================================================
Model Class: HybridConvTransformerDecoder
Total Parameters: 3.93M (3,926,145)
Trainable Parameters: 3.93M (3,926,145)
Non-trainable Parameters: 0 (0)

Model Architecture Summary:
  Device: cuda:0
  Dtype: torch.float32
  Layer Types:
    Conv3d: 7
    DropPath: 3
    Dropout: 6
    GELU: 3
    LayerNorm: 3
    Linear: 11
    MultiHeadCrossAttention: 3
    ResidualConvBlock3D: 3
    RotaryEmbedding: 3
    Upsample: 3
    UpsampleBlock: 3

============================================================
REWARD PREDICTOR MODEL INFORMATION
============================================================
Model Class: RewardPredictor
Total Parameters: 307.20K (307,201)
Trainable Parameters: 307.20K (307,201)
Non-trainable Parameters: 0 (0)

Model Architecture Summary:
  Device: cuda:0
  Dtype: torch.float32
  Layer Types:
    Dropout: 4
    GELU: 3
    LayerNorm: 6
    Linear: 6
    ModuleList: 2
    MultiheadAttention: 3
    NonDynamicallyQuantizableLinear: 3

============================================================
SUMMARY - ALL MODELS COMBINED
============================================================
Total Parameters (All Models): 6.24M (6,239,874)
Trainable Parameters (All Models): 6.24M (6,239,874)
Non-trainable Parameters (All Models): 0 (0)

Parameter Distribution:
  Encoder: 1.20M (19.2%)
  Predictor: 810.62K (13.0%)
  Decoder: 3.93M (62.9%)
  Reward Predictor: 307.20K (4.9%)

ğŸ‰ Model initialization completed successfully!
================================================================================

Running stage: data_collection
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
len(train_dataset):  44277

âœ“ Data collection and loading completed successfully!
âœ“ Training DataLoader ready with 691 batches
âœ“ Validation DataLoader ready with 171 batches

Running stage: encoder_decoder
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_191338-pzgjvlya
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run encoder-decoder-20250713-191337
wandb: â­ï¸ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: ğŸš€ View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/pzgjvlya
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                     batch â–â–‚â–…â–†â–‡â–â–ƒâ–„â–„â–†â–„â–„â–„â–…â–‡â–…â–†â–ˆâ–…â–†â–ƒâ–†â–„â–…â–…â–‡â–‚â–†â–ƒâ–†â–…â–„â–…â–ƒâ–†â–ˆâ–ƒâ–â–„â–†
wandb: batch_reconstruction_loss â–…â–…â–„â–‚â–ƒâ–…â–…â–†â–ƒâ–‚â–â–„â–‚â–â–â–ˆâ–„â–â–„â–‚â–â–„â–â–â–‚â–â–â–â–‚â–‚â–ƒâ–â–â–â–‚â–â–â–‚â–ƒâ–ƒ
wandb:                     epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb:                epoch_time â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚
wandb:             learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: train_reconstruction_loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:   val_reconstruction_loss â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                     batch 690
wandb: batch_reconstruction_loss 3.17172
wandb:                     epoch 14
wandb:                epoch_time 118.26047
wandb:             learning_rate 0.0001
wandb: train_reconstruction_loss 2.72671
wandb:   val_reconstruction_loss 2.43857
wandb: 
wandb: ğŸš€ View run encoder-decoder-20250713-191337 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/pzgjvlya
wandb: â­ï¸ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_191338-pzgjvlya/logs
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/15 - Train Loss: 5.027124, Validation Loss: 3.776051
Epoch 2/15 - Train Loss: 3.592443, Validation Loss: 2.933108
Epoch 3/15 - Train Loss: 3.309948, Validation Loss: 2.840967
Epoch 4/15 - Train Loss: 3.250045, Validation Loss: 2.812804
Epoch 5/15 - Train Loss: 3.152427, Validation Loss: 2.728627
Epoch 6/15 - Train Loss: 3.102329, Validation Loss: 2.742122
Epoch 7/15 - Train Loss: 3.055601, Validation Loss: 2.660407
Epoch 8/15 - Train Loss: 2.999027, Validation Loss: 2.587837
Epoch 9/15 - Train Loss: 2.933245, Validation Loss: 2.532571
Epoch 10/15 - Train Loss: 2.887550, Validation Loss: 2.563705
Epoch 11/15 - Train Loss: 2.855737, Validation Loss: 2.481056
Epoch 12/15 - Train Loss: 2.802744, Validation Loss: 2.475640
Epoch 13/15 - Train Loss: 2.762258, Validation Loss: 2.406723
Epoch 14/15 - Train Loss: 2.728891, Validation Loss: 2.418077
Epoch 15/15 - Train Loss: 2.726708, Validation Loss: 2.438571

Running stage: jepa
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_194400-ebv34z9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jepa-20250713-194359
wandb: â­ï¸ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: ğŸš€ View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/ebv34z9q
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch â–„â–†â–â–…â–ˆâ–‡â–â–„â–„â–‚â–…â–‚â–‚â–ƒâ–†â–‚â–…â–†â–„â–†â–„â–…â–†â–‡â–ˆâ–†â–…â–„â–„â–â–†â–â–â–â–„â–‚â–‚â–„â–„â–…
wandb:    batch_loss â–ˆâ–‡â–†â–…â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–„â–„â–„â–„â–„â–…â–„â–…
wandb:         epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb:    epoch_time â–ˆâ–â–‚â–â–â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–
wandb: learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    train_loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†
wandb:      val_loss â–„â–‚â–â–â–â–â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:         batch 690
wandb:    batch_loss 0.07489
wandb:         epoch 14
wandb:    epoch_time 30.76078
wandb: learning_rate 0.0001
wandb:    train_loss 0.07392
wandb:      val_loss 0.07247
wandb: 
wandb: ğŸš€ View run jepa-20250713-194359 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/ebv34z9q
wandb: â­ï¸ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_194400-ebv34z9q/logs
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/15 - Train Loss: 0.092077, Val Loss: 0.051423
Epoch 2/15 - Train Loss: 0.051748, Val Loss: 0.039440
Epoch 3/15 - Train Loss: 0.041193, Val Loss: 0.034838
Epoch 4/15 - Train Loss: 0.037275, Val Loss: 0.032639
Epoch 5/15 - Train Loss: 0.036047, Val Loss: 0.031955
Epoch 6/15 - Train Loss: 0.036260, Val Loss: 0.033172
Epoch 7/15 - Train Loss: 0.037487, Val Loss: 0.034577
Epoch 8/15 - Train Loss: 0.039657, Val Loss: 0.037672
Epoch 9/15 - Train Loss: 0.042586, Val Loss: 0.039880
Epoch 10/15 - Train Loss: 0.046366, Val Loss: 0.045678
Epoch 11/15 - Train Loss: 0.050760, Val Loss: 0.049054
Epoch 12/15 - Train Loss: 0.055881, Val Loss: 0.055277
Epoch 13/15 - Train Loss: 0.061530, Val Loss: 0.061373
Epoch 14/15 - Train Loss: 0.067515, Val Loss: 0.066110
Epoch 15/15 - Train Loss: 0.073917, Val Loss: 0.072467

Running stage: jepa_decoder
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_195158-8nofaswp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jepa-decoder-20250713-195158
wandb: â­ï¸ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: ğŸš€ View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/8nofaswp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                     batch â–ƒâ–ƒâ–…â–‡â–ƒâ–ƒâ–„â–…â–ˆâ–„â–ˆâ–‚â–ƒâ–„â–ˆâ–ƒâ–…â–†â–„â–†â–‡â–ˆâ–…â–…â–ƒâ–â–‚â–…â–…â–†â–†â–ƒâ–â–‚â–‡â–ƒâ–‡â–‚â–„â–‡
wandb: batch_reconstruction_loss â–ˆâ–‡â–ƒâ–„â–ƒâ–‚â–„â–ƒâ–‚â–„â–‡â–‚â–ƒâ–„â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–„â–ƒâ–‡â–‚â–‚â–‚â–‚â–…â–â–„â–â–â–â–…â–ƒâ–â–ƒâ–‚â–â–‚
wandb:                     epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb:                epoch_time â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: train_reconstruction_loss â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–â–
wandb:   val_reconstruction_loss â–ˆâ–‡â–†â–…â–…â–…â–…â–…â–„â–„â–ƒâ–â–‚â–â–
wandb: 
wandb: Run summary:
wandb:                     batch 690
wandb: batch_reconstruction_loss 2.78151
wandb:                     epoch 14
wandb:                epoch_time 102.92278
wandb:             learning_rate 0.0001
wandb: train_reconstruction_loss 2.55836
wandb:   val_reconstruction_loss 2.20905
wandb: 
wandb: ğŸš€ View run jepa-decoder-20250713-195158 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/8nofaswp
wandb: â­ï¸ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_195158-8nofaswp/logs
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/15 - Train Loss: 4.471642, Val Loss: 3.158221
Epoch 2/15 - Train Loss: 3.438836, Val Loss: 3.007286
Epoch 3/15 - Train Loss: 3.323044, Val Loss: 2.849409
Epoch 4/15 - Train Loss: 3.215938, Val Loss: 2.790933
Epoch 5/15 - Train Loss: 3.174834, Val Loss: 2.777506
Epoch 6/15 - Train Loss: 3.141433, Val Loss: 2.760682
Epoch 7/15 - Train Loss: 3.113816, Val Loss: 2.756390
Epoch 8/15 - Train Loss: 3.075157, Val Loss: 2.701488
Epoch 9/15 - Train Loss: 2.996221, Val Loss: 2.620316
Epoch 10/15 - Train Loss: 2.955938, Val Loss: 2.604707
Epoch 11/15 - Train Loss: 2.850166, Val Loss: 2.457875
Epoch 12/15 - Train Loss: 2.651900, Val Loss: 2.237680
Epoch 13/15 - Train Loss: 2.616217, Val Loss: 2.306366
Epoch 14/15 - Train Loss: 2.576391, Val Loss: 2.210831
Epoch 15/15 - Train Loss: 2.558363, Val Loss: 2.209050

Running stage: reward_predictor
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_201816-yhuj0g8e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reward-predictor-jepa-20250713-201816
wandb: â­ï¸ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: ğŸš€ View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/yhuj0g8e
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch â–â–‚â–ƒâ–‡â–ƒâ–‚â–„â–ˆâ–â–‚â–„â–…â–…â–‡â–„â–„â–‡â–ˆâ–ˆâ–ƒâ–‚â–‚â–…â–†â–‡â–ƒâ–‡â–‚â–„â–†â–‚â–†â–†â–â–…â–†â–†â–ˆâ–„â–‡
wandb:    batch_loss â–‚â–…â–„â–‚â–‚â–‡â–‚â–‚â–…â–‚â–‚â–†â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–â–†â–ƒâ–†â–‡â–â–ƒâ–ˆâ–†â–ƒâ–â–‡â–‚â–…â–…â–„â–†â–ƒâ–ˆâ–‚â–…â–ƒ
wandb:         epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:    epoch_time â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb: learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    train_loss â–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–„â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:      val_loss â–ˆâ–‡â–‡â–„â–ƒâ–„â–ˆâ–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–†â–â–‚
wandb: 
wandb: Run summary:
wandb:      approach jepa
wandb:         batch 690
wandb:    batch_loss 32.64802
wandb:         epoch 19
wandb:    epoch_time 13.78806
wandb: learning_rate 0.0001
wandb:    train_loss 13.28167
wandb:      val_loss 13.68415
wandb: 
wandb: ğŸš€ View run reward-predictor-jepa-20250713-201816 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/yhuj0g8e
wandb: â­ï¸ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_201816-yhuj0g8e/logs
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_202300-n1p1d9h2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reward-predictor-encoder_decoder-20250713-202300
wandb: â­ï¸ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: ğŸš€ View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/n1p1d9h2
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch â–„â–ˆâ–ƒâ–†â–ˆâ–‡â–„â–†â–†â–‚â–…â–‡â–‚â–†â–‡â–…â–…â–ƒâ–…â–ˆâ–…â–‚â–ƒâ–…â–…â–‚â–„â–â–‚â–…â–â–…â–‡â–‚â–„â–â–‚â–ƒâ–…â–…
wandb:    batch_loss â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–‡â–ƒâ–†â–‚â–‚â–…â–ƒâ–†â–ƒâ–ƒâ–â–‚â–â–ƒâ–ƒâ–…â–‚â–…â–‚â–ƒâ–‚â–„â–ˆâ–ƒâ–‚â–ˆâ–…â–‚â–ƒâ–‚â–ƒ
wandb:         epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:    epoch_time â–‡â–„â–â–ƒâ–„â–„â–†â–†â–„â–…â–…â–†â–ˆâ–…â–†â–†â–…â–ƒâ–ƒâ–
wandb: learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    train_loss â–ˆâ–†â–‡â–‡â–†â–…â–†â–…â–„â–…â–„â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:      val_loss â–‡â–ˆâ–‡â–†â–†â–†â–†â–…â–…â–„â–„â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–„
wandb: 
wandb: Run summary:
wandb:      approach encoder_decoder
wandb:         batch 690
wandb:    batch_loss 0.26532
wandb:         epoch 19
wandb:    epoch_time 13.71682
wandb: learning_rate 0.0001
wandb:    train_loss 13.20007
wandb:      val_loss 13.69932
wandb: 
wandb: ğŸš€ View run reward-predictor-encoder_decoder-20250713-202300 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/n1p1d9h2
wandb: â­ï¸ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_202300-n1p1d9h2/logs
Training versions specified in config: both

============================================================
Starting JEPA reward predictor training
============================================================
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/20 - Train Loss: 13.454633, Val Loss: 13.829426
Epoch 2/20 - Train Loss: 13.458188, Val Loss: 13.804416
Epoch 3/20 - Train Loss: 13.423186, Val Loss: 13.804403
Epoch 4/20 - Train Loss: 13.403790, Val Loss: 13.736503
Epoch 5/20 - Train Loss: 13.376508, Val Loss: 13.712599
Epoch 6/20 - Train Loss: 13.352712, Val Loss: 13.734270
Epoch 7/20 - Train Loss: 13.342022, Val Loss: 13.842291
Epoch 8/20 - Train Loss: 13.354320, Val Loss: 13.689805
Epoch 9/20 - Train Loss: 13.350770, Val Loss: 13.701203
Epoch 10/20 - Train Loss: 13.335476, Val Loss: 13.709585
Epoch 11/20 - Train Loss: 13.315642, Val Loss: 13.718362
Epoch 12/20 - Train Loss: 13.329843, Val Loss: 13.668190
Epoch 13/20 - Train Loss: 13.322304, Val Loss: 13.691596
Epoch 14/20 - Train Loss: 13.295438, Val Loss: 13.672726
Epoch 15/20 - Train Loss: 13.309035, Val Loss: 13.657030
Epoch 16/20 - Train Loss: 13.297818, Val Loss: 13.712634
Epoch 17/20 - Train Loss: 13.303404, Val Loss: 13.667305
Epoch 18/20 - Train Loss: 13.296457, Val Loss: 13.776833
Epoch 19/20 - Train Loss: 13.285198, Val Loss: 13.641988
Epoch 20/20 - Train Loss: 13.281674, Val Loss: 13.684152

JEPA training completed successfully!

JEPA training finished.

============================================================
Starting ENCODER_DECODER reward predictor training
============================================================
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/20 - Train Loss: 13.461561, Val Loss: 13.818198
Epoch 2/20 - Train Loss: 13.403397, Val Loss: 13.852906
Epoch 3/20 - Train Loss: 13.425065, Val Loss: 13.821158
Epoch 4/20 - Train Loss: 13.420642, Val Loss: 13.765300
Epoch 5/20 - Train Loss: 13.404161, Val Loss: 13.791364
Epoch 6/20 - Train Loss: 13.364865, Val Loss: 13.779979
Epoch 7/20 - Train Loss: 13.370522, Val Loss: 13.767340
Epoch 8/20 - Train Loss: 13.347279, Val Loss: 13.737225
Epoch 9/20 - Train Loss: 13.328852, Val Loss: 13.731068
Epoch 10/20 - Train Loss: 13.332138, Val Loss: 13.711272
Epoch 11/20 - Train Loss: 13.322699, Val Loss: 13.706067
Epoch 12/20 - Train Loss: 13.331902, Val Loss: 13.745844
Epoch 13/20 - Train Loss: 13.282428, Val Loss: 13.665788
Epoch 14/20 - Train Loss: 13.291334, Val Loss: 13.667505
Epoch 15/20 - Train Loss: 13.281431, Val Loss: 13.656660
Epoch 16/20 - Train Loss: 13.242076, Val Loss: 13.627217
Epoch 17/20 - Train Loss: 13.242675, Val Loss: 13.621522
Epoch 18/20 - Train Loss: 13.212405, Val Loss: 13.595119
Epoch 19/20 - Train Loss: 13.206909, Val Loss: 13.569989
Epoch 20/20 - Train Loss: 13.200070, Val Loss: 13.699317

ENCODER_DECODER training completed successfully!

ENCODER_DECODER training finished.

Running stage: dynamics_reward_predictor
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_202746-hcuf4xka
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dynamics-reward-predictor-jepa-20250713-202746
wandb: â­ï¸ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: ğŸš€ View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/hcuf4xka
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch â–†â–ˆâ–ˆâ–ƒâ–‡â–ƒâ–„â–…â–…â–†â–â–†â–â–‚â–…â–ˆâ–‚â–…â–ˆâ–ƒâ–†â–‚â–…â–‡â–ˆâ–‚â–ƒâ–ƒâ–…â–…â–ƒâ–‡â–ƒâ–…â–ƒâ–ˆâ–‡â–‚â–‚â–†
wandb:    batch_loss â–â–ˆâ–…â–†â–†â–‚â–‚â–ƒâ–ƒâ–â–‡â–ƒâ–ƒâ–‚â–‡â–…â–ƒâ–†â–ƒâ–†â–‚â–ƒâ–‚â–‡â–…â–‚â–†â–‚â–‚â–ƒâ–ƒâ–‚â–…â–ƒâ–‡â–ƒâ–‚â–ƒâ–ƒâ–†
wandb:         epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:    epoch_time â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb: learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    train_loss â–ˆâ–†â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–„â–‚â–â–‚â–‚â–ƒ
wandb:      val_loss â–…â–ƒâ–„â–‚â–ˆâ–‡â–†â–‚â–ƒâ–‚â–‡â–â–â–â–â–â–â–â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:      approach jepa
wandb:         batch 690
wandb:    batch_loss 0.38773
wandb:         epoch 19
wandb:    epoch_time 12.59187
wandb: learning_rate 0.0001
wandb:    train_loss 13.39293
wandb:      val_loss 13.77076
wandb: 
wandb: ğŸš€ View run dynamics-reward-predictor-jepa-20250713-202746 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/hcuf4xka
wandb: â­ï¸ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_202746-hcuf4xka/logs
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_203202-9pqpullv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dynamics-reward-predictor-encoder_decoder-20250713-203202
wandb: â­ï¸ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: ğŸš€ View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/9pqpullv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch â–†â–…â–ˆâ–†â–‚â–…â–…â–…â–‚â–…â–â–â–‚â–ƒâ–„â–‡â–†â–‡â–†â–‚â–…â–‚â–„â–â–…â–ƒâ–ˆâ–â–ƒâ–„â–ˆâ–‚â–„â–†â–â–â–‚â–‡â–‡â–„
wandb:    batch_loss â–‡â–ƒâ–†â–‚â–…â–ƒâ–‚â–ƒâ–‡â–‚â–ƒâ–â–â–‡â–„â–‚â–…â–ƒâ–…â–ƒâ–‚â–„â–…â–â–„â–†â–„â–…â–ˆâ–ƒâ–†â–‚â–â–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚
wandb:         epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:    epoch_time â–…â–‡â–ˆâ–‡â–‡â–…â–…â–†â–†â–‡â–…â–â–„â–‚â–‡â–†â–†â–†â–†â–‡
wandb: learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    train_loss â–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–„â–…â–ƒâ–„â–„â–ƒâ–‚â–ƒâ–â–‚â–â–‚
wandb:      val_loss â–ˆâ–‡â–†â–‡â–…â–†â–„â–„â–…â–„â–†â–ƒâ–‚â–‚â–…â–‚â–‚â–‚â–…â–
wandb: 
wandb: Run summary:
wandb:      approach encoder_decoder
wandb:         batch 690
wandb:    batch_loss 7.04871
wandb:         epoch 19
wandb:    epoch_time 12.6678
wandb: learning_rate 0.0001
wandb:    train_loss 13.30481
wandb:      val_loss 13.62674
wandb: 
wandb: ğŸš€ View run dynamics-reward-predictor-encoder_decoder-20250713-203202 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/9pqpullv
wandb: â­ï¸ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_203202-9pqpullv/logs
Training versions specified in config: both
Starting dynamics reward predictor training with jepa approach...
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/20 - Train Loss: 13.480657, Val Loss: 13.836102
Epoch 2/20 - Train Loss: 13.452809, Val Loss: 13.801751
Epoch 3/20 - Train Loss: 13.408271, Val Loss: 13.809060
Epoch 4/20 - Train Loss: 13.416961, Val Loss: 13.783368
Epoch 5/20 - Train Loss: 13.391550, Val Loss: 13.889939
Epoch 6/20 - Train Loss: 13.397310, Val Loss: 13.875857
Epoch 7/20 - Train Loss: 13.396946, Val Loss: 13.849903
Epoch 8/20 - Train Loss: 13.374545, Val Loss: 13.770240
Epoch 9/20 - Train Loss: 13.401098, Val Loss: 13.795747
Epoch 10/20 - Train Loss: 13.391002, Val Loss: 13.779247
Epoch 11/20 - Train Loss: 13.377516, Val Loss: 13.877977
Epoch 12/20 - Train Loss: 13.398523, Val Loss: 13.760353
Epoch 13/20 - Train Loss: 13.374415, Val Loss: 13.759263
Epoch 14/20 - Train Loss: 13.368090, Val Loss: 13.765994
Epoch 15/20 - Train Loss: 13.405006, Val Loss: 13.763947
Epoch 16/20 - Train Loss: 13.385014, Val Loss: 13.758913
Epoch 17/20 - Train Loss: 13.360801, Val Loss: 13.760317
Epoch 18/20 - Train Loss: 13.369708, Val Loss: 13.762019
Epoch 19/20 - Train Loss: 13.374330, Val Loss: 13.802835
Epoch 20/20 - Train Loss: 13.392933, Val Loss: 13.770761
Dynamics reward predictor training with jepa approach completed!

JEPA training completed successfully!

JEPA training finished.
Starting dynamics reward predictor training with encoder_decoder approach...
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/20 - Train Loss: 13.440298, Val Loss: 13.882257
Epoch 2/20 - Train Loss: 13.409417, Val Loss: 13.829058
Epoch 3/20 - Train Loss: 13.413566, Val Loss: 13.800786
Epoch 4/20 - Train Loss: 13.426273, Val Loss: 13.849851
Epoch 5/20 - Train Loss: 13.394914, Val Loss: 13.755264
Epoch 6/20 - Train Loss: 13.394128, Val Loss: 13.825057
Epoch 7/20 - Train Loss: 13.402715, Val Loss: 13.722078
Epoch 8/20 - Train Loss: 13.380736, Val Loss: 13.735201
Epoch 9/20 - Train Loss: 13.359545, Val Loss: 13.773777
Epoch 10/20 - Train Loss: 13.368771, Val Loss: 13.722272
Epoch 11/20 - Train Loss: 13.323105, Val Loss: 13.811076
Epoch 12/20 - Train Loss: 13.353735, Val Loss: 13.698772
Epoch 13/20 - Train Loss: 13.352393, Val Loss: 13.669220
Epoch 14/20 - Train Loss: 13.321311, Val Loss: 13.679595
Epoch 15/20 - Train Loss: 13.307517, Val Loss: 13.760038
Epoch 16/20 - Train Loss: 13.335335, Val Loss: 13.668599
Epoch 17/20 - Train Loss: 13.279152, Val Loss: 13.654080
Epoch 18/20 - Train Loss: 13.310944, Val Loss: 13.647685
Epoch 19/20 - Train Loss: 13.284710, Val Loss: 13.766324
Epoch 20/20 - Train Loss: 13.304808, Val Loss: 13.626739
Dynamics reward predictor training with encoder_decoder approach completed!

ENCODER_DECODER training completed successfully!

ENCODER_DECODER training finished.

Running stage: representation_metrics
2025-07-13 20:36:20,148 - INFO - ğŸš€ Starting comprehensive representation metrics analysis...
2025-07-13 20:36:20,148 - INFO - Running all available analyses
2025-07-13 20:36:20,148 - INFO - ğŸ“Š Analyze local neighborhood structure preservation using Trustworthiness and Continuity
2025-07-13 20:36:20,245 - INFO - Starting neighborhood_preservation analysis...
2025-07-13 21:05:31,781 - INFO - âœ… neighborhood_preservation analysis completed successfully in 1751.54s
2025-07-13 21:05:31,781 - INFO - ğŸ“Š Evaluate intrinsic dimensionality using Participation Ratio and Two-NN estimation
2025-07-13 21:05:31,781 - INFO - Starting manifold_dimension analysis...
2025-07-13 20:36:27,032 - INFO - Loading validation data...
2025-07-13 20:36:27,279 - INFO - Sampling 10913 states from validation data...
2025-07-13 20:37:07,033 - INFO - Loading jepa encoder...
2025-07-13 20:37:07,071 - INFO - Successfully loaded jepa encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/jepa/best_encoder.pth
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples

Analyzing JEPA for k-values:   0%|          | 0/3 [00:00<?, ?it/s]
Analyzing JEPA for k-values:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [01:41<03:23, 101.55s/it]
Analyzing JEPA for k-values:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [05:21<02:51, 171.27s/it]
Analyzing JEPA for k-values: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [14:06<00:00, 332.52s/it]
Analyzing JEPA for k-values: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [14:06<00:00, 282.01s/it]
2025-07-13 20:51:15,798 - INFO - Loading encoder_decoder encoder...
2025-07-13 20:51:15,839 - INFO - Successfully loaded encoder_decoder encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/encoder_decoder/best_encoder.pth

Analyzing ENCODER_DECODER for k-values:   0%|          | 0/3 [00:00<?, ?it/s]
Analyzing ENCODER_DECODER for k-values:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [01:42<03:24, 102.26s/it]
Analyzing ENCODER_DECODER for k-values:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [05:23<02:52, 172.40s/it]
Analyzing ENCODER_DECODER for k-values: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [14:12<00:00, 335.18s/it]
Analyzing ENCODER_DECODER for k-values: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [14:12<00:00, 284.22s/it]
2025-07-13 21:05:30,362 - INFO - Analysis plots saved to /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/neighborhood_preservation/neighborhood_preservation.png

============================================================
         Neighborhood Preservation Analysis Summary         
============================================================
k    Model               Trustworthiness     Continuity          
------------------------------------------------------------
5    JEPA                0.61860.6213
5    ENCODER_DECODER     0.61710.6198
-------------------------
10   JEPA                0.60830.6082
10   ENCODER_DECODER     0.60490.6062
-------------------------
20   JEPA                0.59780.5987
20   ENCODER_DECODER     0.59270.5949
============================================================

Analysis complete. View results at: /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/neighborhood_preservation/neighborhood_preservation.png
2025-07-13 21:05:45,339 - INFO - Loading validation data...
2025-07-13 21:05:45,591 - INFO - Loading jepa encoder...
2025-07-13 21:05:46,798 - INFO - Successfully loaded jepa encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/jepa/best_encoder.pth
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples

Extracting jepa representations:   0%|          | 0/171 [00:00<?, ?it/s]
Extracting jepa representations:   1%|          | 1/171 [00:01<04:12,  1.49s/it]
Extracting jepa representations:   5%|â–Œ         | 9/171 [00:01<00:21,  7.62it/s]
Extracting jepa representations:  11%|â–ˆ         | 18/171 [00:01<00:09, 16.64it/s]
Extracting jepa representations:  16%|â–ˆâ–Œ        | 27/171 [00:01<00:05, 26.31it/s]
Extracting jepa representations:  21%|â–ˆâ–ˆ        | 36/171 [00:01<00:03, 36.09it/s]
Extracting jepa representations:  26%|â–ˆâ–ˆâ–‹       | 45/171 [00:02<00:02, 45.52it/s]
Extracting jepa representations:  32%|â–ˆâ–ˆâ–ˆâ–      | 54/171 [00:02<00:02, 53.96it/s]
Extracting jepa representations:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 63/171 [00:02<00:01, 61.17it/s]
Extracting jepa representations:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/171 [00:02<00:01, 66.97it/s]
Extracting jepa representations:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 81/171 [00:02<00:01, 71.37it/s]
Extracting jepa representations:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/171 [00:02<00:01, 74.47it/s]
Extracting jepa representations:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 99/171 [00:02<00:00, 76.90it/s]
Extracting jepa representations:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 108/171 [00:02<00:00, 78.39it/s]
Extracting jepa representations:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 117/171 [00:02<00:00, 79.95it/s]
Extracting jepa representations:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 126/171 [00:02<00:00, 81.11it/s]
Extracting jepa representations:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 135/171 [00:03<00:00, 81.44it/s]
Extracting jepa representations:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/171 [00:03<00:00, 82.08it/s]
Extracting jepa representations:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 153/171 [00:03<00:00, 82.40it/s]
Extracting jepa representations:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 162/171 [00:03<00:00, 82.52it/s]
Extracting jepa representations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:03<00:00, 83.13it/s]
Extracting jepa representations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:03<00:00, 48.43it/s]
2025-07-13 21:05:51,052 - INFO - Loading encoder_decoder encoder...
2025-07-13 21:05:51,086 - INFO - Successfully loaded encoder_decoder encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/encoder_decoder/best_encoder.pth

==================================================
Manifold Analysis Results for: JEPA
==================================================
  - Nominal Dimension:     128
  - Participation Ratio:   4.2387
  - 2NN Intrinsic Dim:     0.0944 Â± 0.0032
  - Dimension Usage:       3.3%
==================================================


Extracting encoder_decoder representations:   0%|          | 0/171 [00:00<?, ?it/s]
Extracting encoder_decoder representations:   5%|â–Œ         | 9/171 [00:00<00:01, 81.48it/s]
Extracting encoder_decoder representations:  11%|â–ˆ         | 18/171 [00:00<00:01, 82.49it/s]
Extracting encoder_decoder representations:  16%|â–ˆâ–Œ        | 27/171 [00:00<00:01, 82.80it/s]
Extracting encoder_decoder representations:  21%|â–ˆâ–ˆ        | 36/171 [00:00<00:01, 82.99it/s]
Extracting encoder_decoder representations:  26%|â–ˆâ–ˆâ–‹       | 45/171 [00:00<00:01, 83.15it/s]
Extracting encoder_decoder representations:  32%|â–ˆâ–ˆâ–ˆâ–      | 54/171 [00:00<00:01, 83.15it/s]
Extracting encoder_decoder representations:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 63/171 [00:00<00:01, 83.09it/s]
Extracting encoder_decoder representations:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/171 [00:00<00:01, 83.18it/s]
Extracting encoder_decoder representations:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 81/171 [00:00<00:01, 83.18it/s]
Extracting encoder_decoder representations:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/171 [00:01<00:00, 83.15it/s]
Extracting encoder_decoder representations:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 99/171 [00:01<00:00, 83.22it/s]
Extracting encoder_decoder representations:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 108/171 [00:01<00:00, 83.17it/s]
Extracting encoder_decoder representations:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 117/171 [00:01<00:00, 83.23it/s]
Extracting encoder_decoder representations:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 126/171 [00:01<00:00, 83.24it/s]
Extracting encoder_decoder representations:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 135/171 [00:01<00:00, 83.07it/s]
Extracting encoder_decoder representations:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/171 [00:01<00:00, 82.87it/s]
Extracting encoder_decoder representations:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 153/171 [00:01<00:00, 83.01it/s]
Extracting encoder_decoder representations:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 162/171 [00:01<00:00, 83.03it/s]
Extracting encoder_decoder representations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:02<00:00, 84.26it/s]
Extracting encoder_decoder representations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:02<00:00, 83.25it/s]
/home/3157425/rl-worlds/simple-rl-worlds/src/scripts/representation_metrics/analyse_manifold_dimension.py:365: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout(rect=[0, 0, 1, 0.96])
2025-07-13 21:05:55,745 - INFO - âœ… manifold_dimension analysis completed successfully in 23.96s
2025-07-13 21:05:55,745 - INFO - ğŸ“Š Measure representation smoothness by correlating pixel and latent distances
2025-07-13 21:05:55,746 - INFO - Starting smoothness analysis...
2025-07-13 21:06:09,526 - INFO - âœ… smoothness analysis completed successfully in 13.78s
2025-07-13 21:06:09,526 - INFO - ğŸ“Š Test representation stability under Gaussian noise perturbations
2025-07-13 21:06:09,526 - INFO - Starting robustness analysis...
2025-07-13 21:05:54,682 - INFO - Analysis plots saved to /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/manifold_dimension/manifold_dimension_analysis.png

==================================================
Manifold Analysis Results for: ENCODER_DECODER
==================================================
  - Nominal Dimension:     128
  - Participation Ratio:   2.8763
  - 2NN Intrinsic Dim:     0.0904 Â± 0.0023
  - Dimension Usage:       2.2%
==================================================

Analysis complete. View results at: /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/manifold_dimension/manifold_dimension_analysis.png
2025-07-13 21:05:59,611 - INFO - Loading validation data...
2025-07-13 21:05:59,852 - INFO - Loading jepa encoder...
2025-07-13 21:06:00,998 - INFO - Successfully loaded jepa encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/jepa/best_encoder.pth
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples

Analyzing jepa model:   0%|          | 0/171 [00:00<?, ?it/s]
Analyzing jepa model:   1%|          | 1/171 [00:01<04:08,  1.46s/it]
Analyzing jepa model:   5%|â–         | 8/171 [00:01<00:23,  6.81it/s]
Analyzing jepa model:   9%|â–‰         | 15/171 [00:01<00:11, 13.88it/s]
Analyzing jepa model:  13%|â–ˆâ–        | 22/171 [00:01<00:06, 21.56it/s]
Analyzing jepa model:  17%|â–ˆâ–‹        | 29/171 [00:01<00:04, 29.47it/s]
Analyzing jepa model:  21%|â–ˆâ–ˆ        | 36/171 [00:01<00:03, 36.79it/s]
Analyzing jepa model:  25%|â–ˆâ–ˆâ–Œ       | 43/171 [00:02<00:02, 43.64it/s]
Analyzing jepa model:  29%|â–ˆâ–ˆâ–‰       | 50/171 [00:02<00:02, 49.53it/s]
Analyzing jepa model:  33%|â–ˆâ–ˆâ–ˆâ–      | 57/171 [00:02<00:02, 54.46it/s]
Analyzing jepa model:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 64/171 [00:02<00:01, 58.28it/s]
Analyzing jepa model:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 71/171 [00:02<00:01, 60.79it/s]
Analyzing jepa model:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 78/171 [00:02<00:01, 62.87it/s]
Analyzing jepa model:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 85/171 [00:02<00:01, 64.42it/s]
Analyzing jepa model:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 92/171 [00:02<00:01, 65.64it/s]
Analyzing jepa model:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 99/171 [00:02<00:01, 66.26it/s]
Analyzing jepa model:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 106/171 [00:03<00:00, 66.86it/s]
Analyzing jepa model:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 113/171 [00:03<00:00, 67.66it/s]
Analyzing jepa model:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 120/171 [00:03<00:00, 68.08it/s]
Analyzing jepa model:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 127/171 [00:03<00:00, 68.19it/s]
Analyzing jepa model:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 134/171 [00:03<00:00, 68.53it/s]
Analyzing jepa model:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 141/171 [00:03<00:00, 68.64it/s]
Analyzing jepa model:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 148/171 [00:03<00:00, 68.76it/s]
Analyzing jepa model:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 155/171 [00:03<00:00, 68.65it/s]
Analyzing jepa model:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 162/171 [00:03<00:00, 68.60it/s]
Analyzing jepa model:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 169/171 [00:03<00:00, 68.52it/s]
Analyzing jepa model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:03<00:00, 43.35it/s]
2025-07-13 21:06:04,958 - INFO - Loading encoder_decoder encoder...
2025-07-13 21:06:04,992 - INFO - Successfully loaded encoder_decoder encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/encoder_decoder/best_encoder.pth

==================================================
Analysis Results for: JEPA
==================================================
  - Regression Slope: 0.0007
  - R-squared (RÂ²):   0.5086
  - Pearson Corr (r): 0.7132
==================================================


Analyzing encoder_decoder model:   0%|          | 0/171 [00:00<?, ?it/s]
Analyzing encoder_decoder model:   4%|â–         | 7/171 [00:00<00:02, 66.10it/s]
Analyzing encoder_decoder model:   8%|â–Š         | 14/171 [00:00<00:02, 66.18it/s]
Analyzing encoder_decoder model:  12%|â–ˆâ–        | 21/171 [00:00<00:02, 66.27it/s]
Analyzing encoder_decoder model:  16%|â–ˆâ–‹        | 28/171 [00:00<00:02, 66.47it/s]
Analyzing encoder_decoder model:  20%|â–ˆâ–ˆ        | 35/171 [00:00<00:02, 66.50it/s]
Analyzing encoder_decoder model:  25%|â–ˆâ–ˆâ–       | 42/171 [00:00<00:01, 66.85it/s]
Analyzing encoder_decoder model:  29%|â–ˆâ–ˆâ–Š       | 49/171 [00:00<00:01, 67.10it/s]
Analyzing encoder_decoder model:  33%|â–ˆâ–ˆâ–ˆâ–      | 56/171 [00:00<00:01, 67.38it/s]
Analyzing encoder_decoder model:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 63/171 [00:00<00:01, 67.58it/s]
Analyzing encoder_decoder model:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 70/171 [00:01<00:01, 67.50it/s]
Analyzing encoder_decoder model:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 77/171 [00:01<00:01, 67.03it/s]
Analyzing encoder_decoder model:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 84/171 [00:01<00:01, 67.02it/s]
Analyzing encoder_decoder model:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 91/171 [00:01<00:01, 66.76it/s]
Analyzing encoder_decoder model:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 98/171 [00:01<00:01, 67.06it/s]
Analyzing encoder_decoder model:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/171 [00:01<00:00, 66.92it/s]
Analyzing encoder_decoder model:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 112/171 [00:01<00:00, 67.00it/s]
Analyzing encoder_decoder model:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 119/171 [00:01<00:00, 67.27it/s]
Analyzing encoder_decoder model:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 126/171 [00:01<00:00, 67.28it/s]
Analyzing encoder_decoder model:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 133/171 [00:01<00:00, 66.93it/s]
Analyzing encoder_decoder model:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 140/171 [00:02<00:00, 66.77it/s]
Analyzing encoder_decoder model:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 147/171 [00:02<00:00, 66.75it/s]
Analyzing encoder_decoder model:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 154/171 [00:02<00:00, 66.85it/s]
Analyzing encoder_decoder model:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 161/171 [00:02<00:00, 66.70it/s]
Analyzing encoder_decoder model:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 168/171 [00:02<00:00, 66.75it/s]
Analyzing encoder_decoder model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:02<00:00, 67.09it/s]
2025-07-13 21:06:07,580 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-13 21:06:07,593 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
/home/3157425/rl-worlds/simple-rl-worlds/src/scripts/representation_metrics/analyse_smoothness.py:182: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.
  ax2.set_xticklabels(labels)
2025-07-13 21:06:08,481 - INFO - Analysis plots saved to /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/smoothness_analysis/smoothness_comparison.png

==================================================
Analysis Results for: ENCODER_DECODER
==================================================
  - Regression Slope: 0.0012
  - R-squared (RÂ²):   0.2461
  - Pearson Corr (r): 0.4961
==================================================

Analysis complete. View results at: /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/smoothness_analysis/smoothness_comparison.png
2025-07-13 21:06:13,755 - INFO - Loading validation data...
2025-07-13 21:06:14,000 - INFO - Loading jepa encoder...
2025-07-13 21:06:15,158 - INFO - Successfully loaded jepa encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/jepa/best_encoder.pth
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples

Analyzing jepa model:   0%|          | 0/171 [00:00<?, ?it/s]
Analyzing jepa model:   1%|          | 1/171 [00:01<04:19,  1.53s/it]
Analyzing jepa model:   2%|â–         | 3/171 [00:01<01:13,  2.27it/s]
Analyzing jepa model:   3%|â–         | 5/171 [00:01<00:40,  4.11it/s]
Analyzing jepa model:   4%|â–         | 7/171 [00:01<00:26,  6.07it/s]
Analyzing jepa model:   5%|â–Œ         | 9/171 [00:01<00:20,  8.05it/s]
Analyzing jepa model:   6%|â–‹         | 11/171 [00:02<00:16,  9.92it/s]
Analyzing jepa model:   8%|â–Š         | 13/171 [00:02<00:13, 11.57it/s]
Analyzing jepa model:   9%|â–‰         | 15/171 [00:02<00:12, 12.95it/s]
Analyzing jepa model:  10%|â–‰         | 17/171 [00:02<00:10, 14.09it/s]
Analyzing jepa model:  11%|â–ˆ         | 19/171 [00:02<00:10, 14.98it/s]
Analyzing jepa model:  12%|â–ˆâ–        | 21/171 [00:02<00:09, 15.67it/s]
Analyzing jepa model:  13%|â–ˆâ–        | 23/171 [00:02<00:09, 16.16it/s]
Analyzing jepa model:  15%|â–ˆâ–        | 25/171 [00:02<00:08, 16.54it/s]
Analyzing jepa model:  16%|â–ˆâ–Œ        | 27/171 [00:03<00:08, 16.76it/s]
Analyzing jepa model:  17%|â–ˆâ–‹        | 29/171 [00:03<00:08, 16.89it/s]
Analyzing jepa model:  18%|â–ˆâ–Š        | 31/171 [00:03<00:08, 17.06it/s]
Analyzing jepa model:  19%|â–ˆâ–‰        | 33/171 [00:03<00:08, 17.13it/s]
Analyzing jepa model:  20%|â–ˆâ–ˆ        | 35/171 [00:03<00:07, 17.18it/s]
Analyzing jepa model:  22%|â–ˆâ–ˆâ–       | 37/171 [00:03<00:07, 17.27it/s]
Analyzing jepa model:  23%|â–ˆâ–ˆâ–       | 39/171 [00:03<00:07, 17.31it/s]
Analyzing jepa model:  24%|â–ˆâ–ˆâ–       | 41/171 [00:03<00:07, 17.31it/s]
Analyzing jepa model:  25%|â–ˆâ–ˆâ–Œ       | 43/171 [00:03<00:07, 17.33it/s]
Analyzing jepa model:  26%|â–ˆâ–ˆâ–‹       | 45/171 [00:04<00:07, 17.35it/s]
Analyzing jepa model:  27%|â–ˆâ–ˆâ–‹       | 47/171 [00:04<00:07, 17.33it/s]
Analyzing jepa model:  29%|â–ˆâ–ˆâ–Š       | 49/171 [00:04<00:07, 17.36it/s]
Analyzing jepa model:  30%|â–ˆâ–ˆâ–‰       | 51/171 [00:04<00:06, 17.35it/s]
Analyzing jepa model:  31%|â–ˆâ–ˆâ–ˆ       | 53/171 [00:04<00:06, 17.36it/s]
Analyzing jepa model:  32%|â–ˆâ–ˆâ–ˆâ–      | 55/171 [00:04<00:06, 17.30it/s]
Analyzing jepa model:  33%|â–ˆâ–ˆâ–ˆâ–      | 57/171 [00:04<00:06, 17.34it/s]
Analyzing jepa model:  35%|â–ˆâ–ˆâ–ˆâ–      | 59/171 [00:04<00:06, 17.32it/s]
Analyzing jepa model:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 61/171 [00:04<00:06, 17.34it/s]
Analyzing jepa model:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 63/171 [00:05<00:06, 17.35it/s]
Analyzing jepa model:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 65/171 [00:05<00:06, 17.38it/s]
Analyzing jepa model:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 67/171 [00:05<00:05, 17.40it/s]
Analyzing jepa model:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 69/171 [00:05<00:05, 17.36it/s]
Analyzing jepa model:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 71/171 [00:05<00:05, 17.33it/s]
Analyzing jepa model:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 73/171 [00:05<00:05, 17.31it/s]
Analyzing jepa model:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 75/171 [00:05<00:05, 17.35it/s]
Analyzing jepa model:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 77/171 [00:05<00:05, 17.31it/s]
Analyzing jepa model:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 79/171 [00:06<00:05, 17.30it/s]
Analyzing jepa model:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 81/171 [00:06<00:05, 17.34it/s]
Analyzing jepa model:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 83/171 [00:06<00:05, 17.31it/s]
Analyzing jepa model:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 85/171 [00:06<00:04, 17.31it/s]
Analyzing jepa model:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 87/171 [00:06<00:04, 17.31it/s]
Analyzing jepa model:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 89/171 [00:06<00:04, 17.34it/s]
Analyzing jepa model:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 91/171 [00:06<00:04, 17.38it/s]
Analyzing jepa model:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 93/171 [00:06<00:04, 17.37it/s]
Analyzing jepa model:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 95/171 [00:06<00:04, 17.38it/s]
Analyzing jepa model:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 97/171 [00:07<00:04, 17.40it/s]
Analyzing jepa model:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 99/171 [00:07<00:04, 17.36it/s]
Analyzing jepa model:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 101/171 [00:07<00:04, 17.38it/s]
Analyzing jepa model:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 103/171 [00:07<00:03, 17.39it/s]
Analyzing jepa model:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/171 [00:07<00:03, 17.40it/s]
Analyzing jepa model:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 107/171 [00:07<00:03, 17.41it/s]
Analyzing jepa model:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 109/171 [00:07<00:03, 17.40it/s]
Analyzing jepa model:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 111/171 [00:07<00:03, 17.41it/s]
Analyzing jepa model:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 113/171 [00:07<00:03, 17.41it/s]
Analyzing jepa model:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 115/171 [00:08<00:03, 17.35it/s]
Analyzing jepa model:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 117/171 [00:08<00:03, 17.34it/s]
Analyzing jepa model:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 119/171 [00:08<00:03, 17.30it/s]
Analyzing jepa model:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 121/171 [00:08<00:02, 17.24it/s]
Analyzing jepa model:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 123/171 [00:08<00:02, 17.26it/s]
Analyzing jepa model:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 125/171 [00:08<00:02, 17.31it/s]
Analyzing jepa model:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 127/171 [00:08<00:02, 17.35it/s]
Analyzing jepa model:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 129/171 [00:08<00:02, 17.37it/s]
Analyzing jepa model:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 131/171 [00:09<00:02, 17.33it/s]
Analyzing jepa model:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 133/171 [00:09<00:02, 17.32it/s]
Analyzing jepa model:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 135/171 [00:09<00:02, 17.35it/s]
Analyzing jepa model:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 137/171 [00:09<00:01, 17.31it/s]
Analyzing jepa model:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 139/171 [00:09<00:01, 17.30it/s]
Analyzing jepa model:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 141/171 [00:09<00:01, 17.34it/s]
Analyzing jepa model:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 143/171 [00:09<00:01, 17.35it/s]
Analyzing jepa model:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 145/171 [00:09<00:01, 17.33it/s]
Analyzing jepa model:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 147/171 [00:09<00:01, 17.37it/s]
Analyzing jepa model:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 149/171 [00:10<00:01, 17.39it/s]
Analyzing jepa model:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 151/171 [00:10<00:01, 17.41it/s]
Analyzing jepa model:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 153/171 [00:10<00:01, 17.37it/s]
Analyzing jepa model:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 155/171 [00:10<00:00, 17.39it/s]
Analyzing jepa model:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 157/171 [00:10<00:00, 17.41it/s]
Analyzing jepa model:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 159/171 [00:10<00:00, 17.41it/s]
Analyzing jepa model:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 161/171 [00:10<00:00, 17.36it/s]
Analyzing jepa model:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 163/171 [00:10<00:00, 17.33it/s]
Analyzing jepa model:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 165/171 [00:10<00:00, 17.36it/s]
Analyzing jepa model:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 167/171 [00:11<00:00, 17.38it/s]
Analyzing jepa model:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 169/171 [00:11<00:00, 17.37it/s]
Analyzing jepa model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:11<00:00, 15.13it/s]
2025-07-13 21:06:26,462 - INFO - Loading encoder_decoder encoder...
2025-07-13 21:06:26,497 - INFO - Successfully loaded encoder_decoder encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/encoder_decoder/best_encoder.pth

Analyzing encoder_decoder model:   0%|          | 0/171 [00:00<?, ?it/s]
Analyzing encoder_decoder model:   1%|          | 2/171 [00:00<00:09, 17.29it/s]
Analyzing encoder_decoder model:   2%|â–         | 4/171 [00:00<00:09, 17.30it/s]
Analyzing encoder_decoder model:   4%|â–         | 6/171 [00:00<00:09, 17.34it/s]
Analyzing encoder_decoder model:   5%|â–         | 8/171 [00:00<00:09, 17.26it/s]
Analyzing encoder_decoder model:   6%|â–Œ         | 10/171 [00:00<00:09, 17.26it/s]
Analyzing encoder_decoder model:   7%|â–‹         | 12/171 [00:00<00:09, 17.33it/s]
Analyzing encoder_decoder model:   8%|â–Š         | 14/171 [00:00<00:09, 17.38it/s]
Analyzing encoder_decoder model:   9%|â–‰         | 16/171 [00:00<00:08, 17.41it/s]
Analyzing encoder_decoder model:  11%|â–ˆ         | 18/171 [00:01<00:08, 17.43it/s]
Analyzing encoder_decoder model:  12%|â–ˆâ–        | 20/171 [00:01<00:08, 17.45it/s]
Analyzing encoder_decoder model:  13%|â–ˆâ–        | 22/171 [00:01<00:08, 17.46it/s]
Analyzing encoder_decoder model:  14%|â–ˆâ–        | 24/171 [00:01<00:08, 17.45it/s]
Analyzing encoder_decoder model:  15%|â–ˆâ–Œ        | 26/171 [00:01<00:08, 17.46it/s]
Analyzing encoder_decoder model:  16%|â–ˆâ–‹        | 28/171 [00:01<00:08, 17.46it/s]
Analyzing encoder_decoder model:  18%|â–ˆâ–Š        | 30/171 [00:01<00:08, 17.48it/s]
Analyzing encoder_decoder model:  19%|â–ˆâ–Š        | 32/171 [00:01<00:07, 17.48it/s]
Analyzing encoder_decoder model:  20%|â–ˆâ–‰        | 34/171 [00:01<00:07, 17.47it/s]
Analyzing encoder_decoder model:  21%|â–ˆâ–ˆ        | 36/171 [00:02<00:07, 17.49it/s]
Analyzing encoder_decoder model:  22%|â–ˆâ–ˆâ–       | 38/171 [00:02<00:07, 17.48it/s]
Analyzing encoder_decoder model:  23%|â–ˆâ–ˆâ–       | 40/171 [00:02<00:07, 17.45it/s]
Analyzing encoder_decoder model:  25%|â–ˆâ–ˆâ–       | 42/171 [00:02<00:07, 17.39it/s]
Analyzing encoder_decoder model:  26%|â–ˆâ–ˆâ–Œ       | 44/171 [00:02<00:07, 17.41it/s]
Analyzing encoder_decoder model:  27%|â–ˆâ–ˆâ–‹       | 46/171 [00:02<00:07, 17.44it/s]
Analyzing encoder_decoder model:  28%|â–ˆâ–ˆâ–Š       | 48/171 [00:02<00:07, 17.45it/s]
Analyzing encoder_decoder model:  29%|â–ˆâ–ˆâ–‰       | 50/171 [00:02<00:06, 17.42it/s]
Analyzing encoder_decoder model:  30%|â–ˆâ–ˆâ–ˆ       | 52/171 [00:02<00:06, 17.37it/s]
Analyzing encoder_decoder model:  32%|â–ˆâ–ˆâ–ˆâ–      | 54/171 [00:03<00:06, 17.39it/s]
Analyzing encoder_decoder model:  33%|â–ˆâ–ˆâ–ˆâ–      | 56/171 [00:03<00:06, 17.43it/s]
Analyzing encoder_decoder model:  34%|â–ˆâ–ˆâ–ˆâ–      | 58/171 [00:03<00:06, 17.43it/s]
Analyzing encoder_decoder model:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 60/171 [00:03<00:06, 17.45it/s]
Analyzing encoder_decoder model:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 62/171 [00:03<00:06, 17.47it/s]
Analyzing encoder_decoder model:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 64/171 [00:03<00:06, 17.46it/s]
Analyzing encoder_decoder model:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 66/171 [00:03<00:06, 17.47it/s]
Analyzing encoder_decoder model:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 68/171 [00:03<00:05, 17.47it/s]
Analyzing encoder_decoder model:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 70/171 [00:04<00:05, 17.47it/s]
Analyzing encoder_decoder model:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/171 [00:04<00:05, 17.44it/s]
Analyzing encoder_decoder model:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 74/171 [00:04<00:05, 17.42it/s]
Analyzing encoder_decoder model:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 76/171 [00:04<00:05, 17.43it/s]
Analyzing encoder_decoder model:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 78/171 [00:04<00:05, 17.34it/s]
Analyzing encoder_decoder model:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 80/171 [00:04<00:05, 17.32it/s]
Analyzing encoder_decoder model:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 82/171 [00:04<00:05, 17.37it/s]
Analyzing encoder_decoder model:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 84/171 [00:04<00:05, 17.39it/s]
Analyzing encoder_decoder model:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 86/171 [00:04<00:04, 17.43it/s]
Analyzing encoder_decoder model:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 88/171 [00:05<00:04, 17.44it/s]
Analyzing encoder_decoder model:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/171 [00:05<00:04, 17.41it/s]
Analyzing encoder_decoder model:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 92/171 [00:05<00:04, 17.42it/s]
Analyzing encoder_decoder model:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/171 [00:05<00:04, 17.37it/s]
Analyzing encoder_decoder model:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 96/171 [00:05<00:04, 17.41it/s]
Analyzing encoder_decoder model:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 98/171 [00:05<00:04, 17.43it/s]
Analyzing encoder_decoder model:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 100/171 [00:05<00:04, 17.44it/s]
Analyzing encoder_decoder model:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 102/171 [00:05<00:03, 17.45it/s]
Analyzing encoder_decoder model:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 104/171 [00:05<00:03, 17.45it/s]
Analyzing encoder_decoder model:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 106/171 [00:06<00:03, 17.47it/s]
Analyzing encoder_decoder model:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 108/171 [00:06<00:03, 17.43it/s]
Analyzing encoder_decoder model:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 110/171 [00:06<00:03, 17.43it/s]
Analyzing encoder_decoder model:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 112/171 [00:06<00:03, 17.43it/s]
Analyzing encoder_decoder model:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 114/171 [00:06<00:03, 17.38it/s]
Analyzing encoder_decoder model:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 116/171 [00:06<00:03, 17.41it/s]
Analyzing encoder_decoder model:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 118/171 [00:06<00:03, 17.43it/s]
Analyzing encoder_decoder model:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 120/171 [00:06<00:02, 17.44it/s]
Analyzing encoder_decoder model:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 122/171 [00:07<00:02, 17.46it/s]
Analyzing encoder_decoder model:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 124/171 [00:07<00:02, 17.46it/s]
Analyzing encoder_decoder model:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 126/171 [00:07<00:02, 17.47it/s]
Analyzing encoder_decoder model:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 128/171 [00:07<00:02, 17.48it/s]
Analyzing encoder_decoder model:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 130/171 [00:07<00:02, 17.48it/s]
Analyzing encoder_decoder model:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 132/171 [00:07<00:02, 17.44it/s]
Analyzing encoder_decoder model:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 134/171 [00:07<00:02, 17.38it/s]
Analyzing encoder_decoder model:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 136/171 [00:07<00:02, 17.41it/s]
Analyzing encoder_decoder model:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 138/171 [00:07<00:01, 17.44it/s]
Analyzing encoder_decoder model:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 140/171 [00:08<00:01, 17.45it/s]
Analyzing encoder_decoder model:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 142/171 [00:08<00:01, 17.47it/s]
Analyzing encoder_decoder model:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/171 [00:08<00:01, 17.46it/s]
Analyzing encoder_decoder model:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 146/171 [00:08<00:01, 17.48it/s]
Analyzing encoder_decoder model:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 148/171 [00:08<00:01, 17.48it/s]
Analyzing encoder_decoder model:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 150/171 [00:08<00:01, 17.48it/s]
Analyzing encoder_decoder model:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 152/171 [00:08<00:01, 17.49it/s]
Analyzing encoder_decoder model:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 154/171 [00:08<00:00, 17.48it/s]
Analyzing encoder_decoder model:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 156/171 [00:08<00:00, 17.43it/s]
Analyzing encoder_decoder model:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 158/171 [00:09<00:00, 17.32it/s]
Analyzing encoder_decoder model:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 160/171 [00:09<00:00, 17.36it/s]
Analyzing encoder_decoder model:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 162/171 [00:09<00:00, 17.40it/s]
Analyzing encoder_decoder model:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 164/171 [00:09<00:00, 17.42it/s]
Analyzing encoder_decoder model:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 166/171 [00:09<00:00, 17.39it/s]
Analyzing encoder_decoder model:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 168/171 [00:09<00:00, 17.41it/s]
Analyzing encoder_decoder model:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 170/171 [00:09<00:00, 17.36it/s]
Analyzing encoder_decoder model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:09<00:00, 17.47it/s]
2025-07-13 21:06:36,329 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-13 21:06:36,340 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
/home/3157425/rl-worlds/simple-rl-worlds/src/scripts/representation_metrics/analyse_robustness.py:143: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.
  ax2.set_xticklabels(labels)
2025-07-13 21:06:36,992 - INFO - Analysis plots saved to /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/robustness_analysis/robustness_comparison.png

============================================================
                Robustness Analysis Summary                 
   Mean Latent Distance E||Ï†(sÌƒ) - Ï†(s)|| (Â± Std. Error)    
2025-07-13 21:06:38,028 - INFO - âœ… robustness analysis completed successfully in 28.50s
2025-07-13 21:06:38,028 - INFO - 
============================================================
2025-07-13 21:06:38,028 - INFO - ğŸ REPRESENTATION METRICS ANALYSIS SUMMARY
2025-07-13 21:06:38,028 - INFO - ============================================================
2025-07-13 21:06:38,028 - INFO - â±ï¸  Total duration: 1817.88s
2025-07-13 21:06:38,028 - INFO - âœ… Successful analyses (4): neighborhood_preservation, manifold_dimension, smoothness, robustness
2025-07-13 21:06:38,028 - INFO - ğŸ‰ All analyses completed successfully!
2025-07-13 21:06:38,028 - INFO - 
ğŸ“ Check the evaluation_plots/ directory for generated visualizations
2025-07-13 21:06:38,028 - INFO - ğŸ“Š Generated plots can be found in:
2025-07-13 21:06:38,028 - INFO -    â€¢ evaluation_plots/neighborhood_preservation/
2025-07-13 21:06:38,028 - INFO -    â€¢ evaluation_plots/manifold_dimension/
2025-07-13 21:06:38,028 - INFO -    â€¢ evaluation_plots/smoothness_analysis/
2025-07-13 21:06:38,028 - INFO -    â€¢ evaluation_plots/robustness_analysis/
============================================================
Noise Level    Model               Mean Distance            
------------------------------------------------------------
0.01           JEPA                4.3440 Â± 0.0066
0.01           ENCODER_DECODER     9.5369 Â± 0.0267
------------------------------------------------------------
0.05           JEPA                4.2337 Â± 0.0067
0.05           ENCODER_DECODER     9.2440 Â± 0.0256
------------------------------------------------------------
0.10           JEPA                4.0812 Â± 0.0069
0.10           ENCODER_DECODER     8.9502 Â± 0.0243
------------------------------------------------------------
0.15           JEPA                3.8647 Â± 0.0071
0.15           ENCODER_DECODER     8.7409 Â± 0.0233
------------------------------------------------------------
0.20           JEPA                3.6306 Â± 0.0073
0.20           ENCODER_DECODER     8.6659 Â± 0.0232
------------------------------------------------------------

Analysis complete. View results at: /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/robustness_analysis/robustness_comparison.png
