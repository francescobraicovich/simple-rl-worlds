Running on node: gnode02
Using 4 CPU cores
CUDA devices available:
Sun Jul 13 19:13:10 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100 80GB PCIe           On | 00000000:65:00.0 Off |                    0 |
| N/A   42C    P0               45W / 300W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+

Running stage: model_init
🚀 MODEL INITIALIZATION AND PARAMETER ANALYSIS
================================================================================
Using device: cuda

📊 Initializing models...

1. Initializing Encoder...
2. Initializing Predictor...
3. Initializing Decoder...
4. Initializing Reward Predictor...

✅ All models initialized successfully!

============================================================
ENCODER MODEL INFORMATION
============================================================
Model Class: VideoViT
Total Parameters: 1.20M (1,195,904)
Trainable Parameters: 1.20M (1,195,904)
Non-trainable Parameters: 0 (0)

Model Architecture Summary:
  Device: cuda:0
  Dtype: torch.float32
  Layer Types:
    DropPath: 12
    Dropout: 18
    GELU: 6
    LayerNorm: 13
    Linear: 24
    MLP: 6
    MultiHeadSelfAttention: 6
    RotaryEmbedding: 6
    TransformerBlock: 6

============================================================
PREDICTOR MODEL INFORMATION
============================================================
Model Class: LatentDynamicsPredictor
Total Parameters: 810.62K (810,624)
Trainable Parameters: 810.62K (810,624)
Non-trainable Parameters: 0 (0)

Model Architecture Summary:
  Device: cuda:0
  Dtype: torch.float32
  Layer Types:
    DropPath: 8
    Dropout: 12
    GELU: 4
    LayerNorm: 9
    Linear: 17
    MLP: 4
    MultiHeadSelfAttention: 4
    RotaryEmbedding: 4
    TransformerBlock: 4

============================================================
DECODER MODEL INFORMATION
============================================================
Model Class: HybridConvTransformerDecoder
Total Parameters: 3.93M (3,926,145)
Trainable Parameters: 3.93M (3,926,145)
Non-trainable Parameters: 0 (0)

Model Architecture Summary:
  Device: cuda:0
  Dtype: torch.float32
  Layer Types:
    Conv3d: 7
    DropPath: 3
    Dropout: 6
    GELU: 3
    LayerNorm: 3
    Linear: 11
    MultiHeadCrossAttention: 3
    ResidualConvBlock3D: 3
    RotaryEmbedding: 3
    Upsample: 3
    UpsampleBlock: 3

============================================================
REWARD PREDICTOR MODEL INFORMATION
============================================================
Model Class: RewardPredictor
Total Parameters: 307.20K (307,201)
Trainable Parameters: 307.20K (307,201)
Non-trainable Parameters: 0 (0)

Model Architecture Summary:
  Device: cuda:0
  Dtype: torch.float32
  Layer Types:
    Dropout: 4
    GELU: 3
    LayerNorm: 6
    Linear: 6
    ModuleList: 2
    MultiheadAttention: 3
    NonDynamicallyQuantizableLinear: 3

============================================================
SUMMARY - ALL MODELS COMBINED
============================================================
Total Parameters (All Models): 6.24M (6,239,874)
Trainable Parameters (All Models): 6.24M (6,239,874)
Non-trainable Parameters (All Models): 0 (0)

Parameter Distribution:
  Encoder: 1.20M (19.2%)
  Predictor: 810.62K (13.0%)
  Decoder: 3.93M (62.9%)
  Reward Predictor: 307.20K (4.9%)

🎉 Model initialization completed successfully!
================================================================================

Running stage: data_collection
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
len(train_dataset):  44277

✓ Data collection and loading completed successfully!
✓ Training DataLoader ready with 691 batches
✓ Validation DataLoader ready with 171 batches

Running stage: encoder_decoder
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_191338-pzgjvlya
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run encoder-decoder-20250713-191337
wandb: ⭐️ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: 🚀 View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/pzgjvlya
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                     batch ▁▂▅▆▇▁▃▄▄▆▄▄▄▅▇▅▆█▅▆▃▆▄▅▅▇▂▆▃▆▅▄▅▃▆█▃▁▄▆
wandb: batch_reconstruction_loss ▅▅▄▂▃▅▅▆▃▂▁▄▂▁▁█▄▁▄▂▁▄▁▁▂▁▁▁▂▂▃▁▁▁▂▁▁▂▃▃
wandb:                     epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█
wandb:                epoch_time █▂▂▂▂▂▂▂▁▁▁▁▁▂▂
wandb:             learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_reconstruction_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:   val_reconstruction_loss █▄▃▃▃▃▂▂▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                     batch 690
wandb: batch_reconstruction_loss 3.17172
wandb:                     epoch 14
wandb:                epoch_time 118.26047
wandb:             learning_rate 0.0001
wandb: train_reconstruction_loss 2.72671
wandb:   val_reconstruction_loss 2.43857
wandb: 
wandb: 🚀 View run encoder-decoder-20250713-191337 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/pzgjvlya
wandb: ⭐️ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_191338-pzgjvlya/logs
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/15 - Train Loss: 5.027124, Validation Loss: 3.776051
Epoch 2/15 - Train Loss: 3.592443, Validation Loss: 2.933108
Epoch 3/15 - Train Loss: 3.309948, Validation Loss: 2.840967
Epoch 4/15 - Train Loss: 3.250045, Validation Loss: 2.812804
Epoch 5/15 - Train Loss: 3.152427, Validation Loss: 2.728627
Epoch 6/15 - Train Loss: 3.102329, Validation Loss: 2.742122
Epoch 7/15 - Train Loss: 3.055601, Validation Loss: 2.660407
Epoch 8/15 - Train Loss: 2.999027, Validation Loss: 2.587837
Epoch 9/15 - Train Loss: 2.933245, Validation Loss: 2.532571
Epoch 10/15 - Train Loss: 2.887550, Validation Loss: 2.563705
Epoch 11/15 - Train Loss: 2.855737, Validation Loss: 2.481056
Epoch 12/15 - Train Loss: 2.802744, Validation Loss: 2.475640
Epoch 13/15 - Train Loss: 2.762258, Validation Loss: 2.406723
Epoch 14/15 - Train Loss: 2.728891, Validation Loss: 2.418077
Epoch 15/15 - Train Loss: 2.726708, Validation Loss: 2.438571

Running stage: jepa
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_194400-ebv34z9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jepa-20250713-194359
wandb: ⭐️ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: 🚀 View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/ebv34z9q
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch ▄▆▁▅█▇▁▄▄▂▅▂▂▃▆▂▅▆▄▆▄▅▆▇█▆▅▄▄▁▆▁▁▁▄▂▂▄▄▅
wandb:    batch_loss █▇▆▅▃▂▂▂▃▂▂▁▁▂▁▂▂▂▂▁▂▂▂▂▂▂▃▃▃▂▂▄▄▄▄▄▄▅▄▅
wandb:         epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█
wandb:    epoch_time █▁▂▁▁▂▁▁▂▂▂▁▂▁▁
wandb: learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▃▂▁▁▁▁▁▂▂▃▃▄▅▆
wandb:      val_loss ▄▂▁▁▁▁▁▂▂▃▄▅▆▇█
wandb: 
wandb: Run summary:
wandb:         batch 690
wandb:    batch_loss 0.07489
wandb:         epoch 14
wandb:    epoch_time 30.76078
wandb: learning_rate 0.0001
wandb:    train_loss 0.07392
wandb:      val_loss 0.07247
wandb: 
wandb: 🚀 View run jepa-20250713-194359 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/ebv34z9q
wandb: ⭐️ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_194400-ebv34z9q/logs
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/15 - Train Loss: 0.092077, Val Loss: 0.051423
Epoch 2/15 - Train Loss: 0.051748, Val Loss: 0.039440
Epoch 3/15 - Train Loss: 0.041193, Val Loss: 0.034838
Epoch 4/15 - Train Loss: 0.037275, Val Loss: 0.032639
Epoch 5/15 - Train Loss: 0.036047, Val Loss: 0.031955
Epoch 6/15 - Train Loss: 0.036260, Val Loss: 0.033172
Epoch 7/15 - Train Loss: 0.037487, Val Loss: 0.034577
Epoch 8/15 - Train Loss: 0.039657, Val Loss: 0.037672
Epoch 9/15 - Train Loss: 0.042586, Val Loss: 0.039880
Epoch 10/15 - Train Loss: 0.046366, Val Loss: 0.045678
Epoch 11/15 - Train Loss: 0.050760, Val Loss: 0.049054
Epoch 12/15 - Train Loss: 0.055881, Val Loss: 0.055277
Epoch 13/15 - Train Loss: 0.061530, Val Loss: 0.061373
Epoch 14/15 - Train Loss: 0.067515, Val Loss: 0.066110
Epoch 15/15 - Train Loss: 0.073917, Val Loss: 0.072467

Running stage: jepa_decoder
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_195158-8nofaswp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jepa-decoder-20250713-195158
wandb: ⭐️ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: 🚀 View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/8nofaswp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                     batch ▃▃▅▇▃▃▄▅█▄█▂▃▄█▃▅▆▄▆▇█▅▅▃▁▂▅▅▆▆▃▁▂▇▃▇▂▄▇
wandb: batch_reconstruction_loss █▇▃▄▃▂▄▃▂▄▇▂▃▄▃▂▂▂▃▂▄▃▇▂▂▂▂▅▁▄▁▁▁▅▃▁▃▂▁▂
wandb:                     epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█
wandb:                epoch_time █▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_reconstruction_loss █▄▄▃▃▃▃▃▃▂▂▁▁▁▁
wandb:   val_reconstruction_loss █▇▆▅▅▅▅▅▄▄▃▁▂▁▁
wandb: 
wandb: Run summary:
wandb:                     batch 690
wandb: batch_reconstruction_loss 2.78151
wandb:                     epoch 14
wandb:                epoch_time 102.92278
wandb:             learning_rate 0.0001
wandb: train_reconstruction_loss 2.55836
wandb:   val_reconstruction_loss 2.20905
wandb: 
wandb: 🚀 View run jepa-decoder-20250713-195158 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/8nofaswp
wandb: ⭐️ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_195158-8nofaswp/logs
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/15 - Train Loss: 4.471642, Val Loss: 3.158221
Epoch 2/15 - Train Loss: 3.438836, Val Loss: 3.007286
Epoch 3/15 - Train Loss: 3.323044, Val Loss: 2.849409
Epoch 4/15 - Train Loss: 3.215938, Val Loss: 2.790933
Epoch 5/15 - Train Loss: 3.174834, Val Loss: 2.777506
Epoch 6/15 - Train Loss: 3.141433, Val Loss: 2.760682
Epoch 7/15 - Train Loss: 3.113816, Val Loss: 2.756390
Epoch 8/15 - Train Loss: 3.075157, Val Loss: 2.701488
Epoch 9/15 - Train Loss: 2.996221, Val Loss: 2.620316
Epoch 10/15 - Train Loss: 2.955938, Val Loss: 2.604707
Epoch 11/15 - Train Loss: 2.850166, Val Loss: 2.457875
Epoch 12/15 - Train Loss: 2.651900, Val Loss: 2.237680
Epoch 13/15 - Train Loss: 2.616217, Val Loss: 2.306366
Epoch 14/15 - Train Loss: 2.576391, Val Loss: 2.210831
Epoch 15/15 - Train Loss: 2.558363, Val Loss: 2.209050

Running stage: reward_predictor
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_201816-yhuj0g8e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reward-predictor-jepa-20250713-201816
wandb: ⭐️ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: 🚀 View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/yhuj0g8e
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch ▁▂▃▇▃▂▄█▁▂▄▅▅▇▄▄▇██▃▂▂▅▆▇▃▇▂▄▆▂▆▆▁▅▆▆█▄▇
wandb:    batch_loss ▂▅▄▂▂▇▂▂▅▂▂▆▃▃▄▃▂▂▁▆▃▆▇▁▃█▆▃▁▇▂▅▅▄▆▃█▂▅▃
wandb:         epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:    epoch_time █▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb: learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss ██▇▆▅▄▃▄▄▃▂▃▃▂▂▂▂▂▁▁
wandb:      val_loss █▇▇▄▃▄█▃▃▃▄▂▃▂▂▃▂▆▁▂
wandb: 
wandb: Run summary:
wandb:      approach jepa
wandb:         batch 690
wandb:    batch_loss 32.64802
wandb:         epoch 19
wandb:    epoch_time 13.78806
wandb: learning_rate 0.0001
wandb:    train_loss 13.28167
wandb:      val_loss 13.68415
wandb: 
wandb: 🚀 View run reward-predictor-jepa-20250713-201816 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/yhuj0g8e
wandb: ⭐️ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_201816-yhuj0g8e/logs
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_202300-n1p1d9h2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reward-predictor-encoder_decoder-20250713-202300
wandb: ⭐️ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: 🚀 View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/n1p1d9h2
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch ▄█▃▆█▇▄▆▆▂▅▇▂▆▇▅▅▃▅█▅▂▃▅▅▂▄▁▂▅▁▅▇▂▄▁▂▃▅▅
wandb:    batch_loss ▃▂▂▃▂▃▃▁▂▇▃▆▂▂▅▃▆▃▃▁▂▁▃▃▅▂▅▂▃▂▄█▃▂█▅▂▃▂▃
wandb:         epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:    epoch_time ▇▄▁▃▄▄▆▆▄▅▅▆█▅▆▆▅▃▃▁
wandb: learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▆▇▇▆▅▆▅▄▅▄▅▃▃▃▂▂▁▁▁
wandb:      val_loss ▇█▇▆▆▆▆▅▅▄▄▅▃▃▃▂▂▂▁▄
wandb: 
wandb: Run summary:
wandb:      approach encoder_decoder
wandb:         batch 690
wandb:    batch_loss 0.26532
wandb:         epoch 19
wandb:    epoch_time 13.71682
wandb: learning_rate 0.0001
wandb:    train_loss 13.20007
wandb:      val_loss 13.69932
wandb: 
wandb: 🚀 View run reward-predictor-encoder_decoder-20250713-202300 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/n1p1d9h2
wandb: ⭐️ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_202300-n1p1d9h2/logs
Training versions specified in config: both

============================================================
Starting JEPA reward predictor training
============================================================
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/20 - Train Loss: 13.454633, Val Loss: 13.829426
Epoch 2/20 - Train Loss: 13.458188, Val Loss: 13.804416
Epoch 3/20 - Train Loss: 13.423186, Val Loss: 13.804403
Epoch 4/20 - Train Loss: 13.403790, Val Loss: 13.736503
Epoch 5/20 - Train Loss: 13.376508, Val Loss: 13.712599
Epoch 6/20 - Train Loss: 13.352712, Val Loss: 13.734270
Epoch 7/20 - Train Loss: 13.342022, Val Loss: 13.842291
Epoch 8/20 - Train Loss: 13.354320, Val Loss: 13.689805
Epoch 9/20 - Train Loss: 13.350770, Val Loss: 13.701203
Epoch 10/20 - Train Loss: 13.335476, Val Loss: 13.709585
Epoch 11/20 - Train Loss: 13.315642, Val Loss: 13.718362
Epoch 12/20 - Train Loss: 13.329843, Val Loss: 13.668190
Epoch 13/20 - Train Loss: 13.322304, Val Loss: 13.691596
Epoch 14/20 - Train Loss: 13.295438, Val Loss: 13.672726
Epoch 15/20 - Train Loss: 13.309035, Val Loss: 13.657030
Epoch 16/20 - Train Loss: 13.297818, Val Loss: 13.712634
Epoch 17/20 - Train Loss: 13.303404, Val Loss: 13.667305
Epoch 18/20 - Train Loss: 13.296457, Val Loss: 13.776833
Epoch 19/20 - Train Loss: 13.285198, Val Loss: 13.641988
Epoch 20/20 - Train Loss: 13.281674, Val Loss: 13.684152

JEPA training completed successfully!

JEPA training finished.

============================================================
Starting ENCODER_DECODER reward predictor training
============================================================
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/20 - Train Loss: 13.461561, Val Loss: 13.818198
Epoch 2/20 - Train Loss: 13.403397, Val Loss: 13.852906
Epoch 3/20 - Train Loss: 13.425065, Val Loss: 13.821158
Epoch 4/20 - Train Loss: 13.420642, Val Loss: 13.765300
Epoch 5/20 - Train Loss: 13.404161, Val Loss: 13.791364
Epoch 6/20 - Train Loss: 13.364865, Val Loss: 13.779979
Epoch 7/20 - Train Loss: 13.370522, Val Loss: 13.767340
Epoch 8/20 - Train Loss: 13.347279, Val Loss: 13.737225
Epoch 9/20 - Train Loss: 13.328852, Val Loss: 13.731068
Epoch 10/20 - Train Loss: 13.332138, Val Loss: 13.711272
Epoch 11/20 - Train Loss: 13.322699, Val Loss: 13.706067
Epoch 12/20 - Train Loss: 13.331902, Val Loss: 13.745844
Epoch 13/20 - Train Loss: 13.282428, Val Loss: 13.665788
Epoch 14/20 - Train Loss: 13.291334, Val Loss: 13.667505
Epoch 15/20 - Train Loss: 13.281431, Val Loss: 13.656660
Epoch 16/20 - Train Loss: 13.242076, Val Loss: 13.627217
Epoch 17/20 - Train Loss: 13.242675, Val Loss: 13.621522
Epoch 18/20 - Train Loss: 13.212405, Val Loss: 13.595119
Epoch 19/20 - Train Loss: 13.206909, Val Loss: 13.569989
Epoch 20/20 - Train Loss: 13.200070, Val Loss: 13.699317

ENCODER_DECODER training completed successfully!

ENCODER_DECODER training finished.

Running stage: dynamics_reward_predictor
wandb: Currently logged in as: francesco-braicovich (francesco-braicovich-bocconi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_202746-hcuf4xka
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dynamics-reward-predictor-jepa-20250713-202746
wandb: ⭐️ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: 🚀 View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/hcuf4xka
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch ▆██▃▇▃▄▅▅▆▁▆▁▂▅█▂▅█▃▆▂▅▇█▂▃▃▅▅▃▇▃▅▃█▇▂▂▆
wandb:    batch_loss ▁█▅▆▆▂▂▃▃▁▇▃▃▂▇▅▃▆▃▆▂▃▂▇▅▂▆▂▂▃▃▂▅▃▇▃▂▃▃▆
wandb:         epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:    epoch_time █▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb: learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▆▄▄▃▃▃▂▃▃▂▃▂▁▄▂▁▂▂▃
wandb:      val_loss ▅▃▄▂█▇▆▂▃▂▇▁▁▁▁▁▁▁▃▂
wandb: 
wandb: Run summary:
wandb:      approach jepa
wandb:         batch 690
wandb:    batch_loss 0.38773
wandb:         epoch 19
wandb:    epoch_time 12.59187
wandb: learning_rate 0.0001
wandb:    train_loss 13.39293
wandb:      val_loss 13.77076
wandb: 
wandb: 🚀 View run dynamics-reward-predictor-jepa-20250713-202746 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/hcuf4xka
wandb: ⭐️ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_202746-hcuf4xka/logs
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/3157425/rl-worlds/simple-rl-worlds/wandb/run-20250713_203202-9pqpullv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dynamics-reward-predictor-encoder_decoder-20250713-203202
wandb: ⭐️ View project at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: 🚀 View run at https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/9pqpullv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         batch ▆▅█▆▂▅▅▅▂▅▁▁▂▃▄▇▆▇▆▂▅▂▄▁▅▃█▁▃▄█▂▄▆▁▁▂▇▇▄
wandb:    batch_loss ▇▃▆▂▅▃▂▃▇▂▃▁▁▇▄▂▅▃▅▃▂▄▅▁▄▆▄▅█▃▆▂▁▁▃▃▂▃▃▂
wandb:         epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:    epoch_time ▅▇█▇▇▅▅▆▆▇▅▁▄▂▇▆▆▆▆▇
wandb: learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▇▇▇▆▆▆▅▄▅▃▄▄▃▂▃▁▂▁▂
wandb:      val_loss █▇▆▇▅▆▄▄▅▄▆▃▂▂▅▂▂▂▅▁
wandb: 
wandb: Run summary:
wandb:      approach encoder_decoder
wandb:         batch 690
wandb:    batch_loss 7.04871
wandb:         epoch 19
wandb:    epoch_time 12.6678
wandb: learning_rate 0.0001
wandb:    train_loss 13.30481
wandb:      val_loss 13.62674
wandb: 
wandb: 🚀 View run dynamics-reward-predictor-encoder_decoder-20250713-203202 at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds/runs/9pqpullv
wandb: ⭐️ View project at: https://wandb.ai/francesco-braicovich-bocconi-university/simple-rl-worlds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250713_203202-9pqpullv/logs
Training versions specified in config: both
Starting dynamics reward predictor training with jepa approach...
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/20 - Train Loss: 13.480657, Val Loss: 13.836102
Epoch 2/20 - Train Loss: 13.452809, Val Loss: 13.801751
Epoch 3/20 - Train Loss: 13.408271, Val Loss: 13.809060
Epoch 4/20 - Train Loss: 13.416961, Val Loss: 13.783368
Epoch 5/20 - Train Loss: 13.391550, Val Loss: 13.889939
Epoch 6/20 - Train Loss: 13.397310, Val Loss: 13.875857
Epoch 7/20 - Train Loss: 13.396946, Val Loss: 13.849903
Epoch 8/20 - Train Loss: 13.374545, Val Loss: 13.770240
Epoch 9/20 - Train Loss: 13.401098, Val Loss: 13.795747
Epoch 10/20 - Train Loss: 13.391002, Val Loss: 13.779247
Epoch 11/20 - Train Loss: 13.377516, Val Loss: 13.877977
Epoch 12/20 - Train Loss: 13.398523, Val Loss: 13.760353
Epoch 13/20 - Train Loss: 13.374415, Val Loss: 13.759263
Epoch 14/20 - Train Loss: 13.368090, Val Loss: 13.765994
Epoch 15/20 - Train Loss: 13.405006, Val Loss: 13.763947
Epoch 16/20 - Train Loss: 13.385014, Val Loss: 13.758913
Epoch 17/20 - Train Loss: 13.360801, Val Loss: 13.760317
Epoch 18/20 - Train Loss: 13.369708, Val Loss: 13.762019
Epoch 19/20 - Train Loss: 13.374330, Val Loss: 13.802835
Epoch 20/20 - Train Loss: 13.392933, Val Loss: 13.770761
Dynamics reward predictor training with jepa approach completed!

JEPA training completed successfully!

JEPA training finished.
Starting dynamics reward predictor training with encoder_decoder approach...
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples
Epoch 1/20 - Train Loss: 13.440298, Val Loss: 13.882257
Epoch 2/20 - Train Loss: 13.409417, Val Loss: 13.829058
Epoch 3/20 - Train Loss: 13.413566, Val Loss: 13.800786
Epoch 4/20 - Train Loss: 13.426273, Val Loss: 13.849851
Epoch 5/20 - Train Loss: 13.394914, Val Loss: 13.755264
Epoch 6/20 - Train Loss: 13.394128, Val Loss: 13.825057
Epoch 7/20 - Train Loss: 13.402715, Val Loss: 13.722078
Epoch 8/20 - Train Loss: 13.380736, Val Loss: 13.735201
Epoch 9/20 - Train Loss: 13.359545, Val Loss: 13.773777
Epoch 10/20 - Train Loss: 13.368771, Val Loss: 13.722272
Epoch 11/20 - Train Loss: 13.323105, Val Loss: 13.811076
Epoch 12/20 - Train Loss: 13.353735, Val Loss: 13.698772
Epoch 13/20 - Train Loss: 13.352393, Val Loss: 13.669220
Epoch 14/20 - Train Loss: 13.321311, Val Loss: 13.679595
Epoch 15/20 - Train Loss: 13.307517, Val Loss: 13.760038
Epoch 16/20 - Train Loss: 13.335335, Val Loss: 13.668599
Epoch 17/20 - Train Loss: 13.279152, Val Loss: 13.654080
Epoch 18/20 - Train Loss: 13.310944, Val Loss: 13.647685
Epoch 19/20 - Train Loss: 13.284710, Val Loss: 13.766324
Epoch 20/20 - Train Loss: 13.304808, Val Loss: 13.626739
Dynamics reward predictor training with encoder_decoder approach completed!

ENCODER_DECODER training completed successfully!

ENCODER_DECODER training finished.

Running stage: representation_metrics
2025-07-13 20:36:20,148 - INFO - 🚀 Starting comprehensive representation metrics analysis...
2025-07-13 20:36:20,148 - INFO - Running all available analyses
2025-07-13 20:36:20,148 - INFO - 📊 Analyze local neighborhood structure preservation using Trustworthiness and Continuity
2025-07-13 20:36:20,245 - INFO - Starting neighborhood_preservation analysis...
2025-07-13 21:05:31,781 - INFO - ✅ neighborhood_preservation analysis completed successfully in 1751.54s
2025-07-13 21:05:31,781 - INFO - 📊 Evaluate intrinsic dimensionality using Participation Ratio and Two-NN estimation
2025-07-13 21:05:31,781 - INFO - Starting manifold_dimension analysis...
2025-07-13 20:36:27,032 - INFO - Loading validation data...
2025-07-13 20:36:27,279 - INFO - Sampling 10913 states from validation data...
2025-07-13 20:37:07,033 - INFO - Loading jepa encoder...
2025-07-13 20:37:07,071 - INFO - Successfully loaded jepa encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/jepa/best_encoder.pth
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples

Analyzing JEPA for k-values:   0%|          | 0/3 [00:00<?, ?it/s]
Analyzing JEPA for k-values:  33%|███▎      | 1/3 [01:41<03:23, 101.55s/it]
Analyzing JEPA for k-values:  67%|██████▋   | 2/3 [05:21<02:51, 171.27s/it]
Analyzing JEPA for k-values: 100%|██████████| 3/3 [14:06<00:00, 332.52s/it]
Analyzing JEPA for k-values: 100%|██████████| 3/3 [14:06<00:00, 282.01s/it]
2025-07-13 20:51:15,798 - INFO - Loading encoder_decoder encoder...
2025-07-13 20:51:15,839 - INFO - Successfully loaded encoder_decoder encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/encoder_decoder/best_encoder.pth

Analyzing ENCODER_DECODER for k-values:   0%|          | 0/3 [00:00<?, ?it/s]
Analyzing ENCODER_DECODER for k-values:  33%|███▎      | 1/3 [01:42<03:24, 102.26s/it]
Analyzing ENCODER_DECODER for k-values:  67%|██████▋   | 2/3 [05:23<02:52, 172.40s/it]
Analyzing ENCODER_DECODER for k-values: 100%|██████████| 3/3 [14:12<00:00, 335.18s/it]
Analyzing ENCODER_DECODER for k-values: 100%|██████████| 3/3 [14:12<00:00, 284.22s/it]
2025-07-13 21:05:30,362 - INFO - Analysis plots saved to /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/neighborhood_preservation/neighborhood_preservation.png

============================================================
         Neighborhood Preservation Analysis Summary         
============================================================
k    Model               Trustworthiness     Continuity          
------------------------------------------------------------
5    JEPA                0.61860.6213
5    ENCODER_DECODER     0.61710.6198
-------------------------
10   JEPA                0.60830.6082
10   ENCODER_DECODER     0.60490.6062
-------------------------
20   JEPA                0.59780.5987
20   ENCODER_DECODER     0.59270.5949
============================================================

Analysis complete. View results at: /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/neighborhood_preservation/neighborhood_preservation.png
2025-07-13 21:05:45,339 - INFO - Loading validation data...
2025-07-13 21:05:45,591 - INFO - Loading jepa encoder...
2025-07-13 21:05:46,798 - INFO - Successfully loaded jepa encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/jepa/best_encoder.pth
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples

Extracting jepa representations:   0%|          | 0/171 [00:00<?, ?it/s]
Extracting jepa representations:   1%|          | 1/171 [00:01<04:12,  1.49s/it]
Extracting jepa representations:   5%|▌         | 9/171 [00:01<00:21,  7.62it/s]
Extracting jepa representations:  11%|█         | 18/171 [00:01<00:09, 16.64it/s]
Extracting jepa representations:  16%|█▌        | 27/171 [00:01<00:05, 26.31it/s]
Extracting jepa representations:  21%|██        | 36/171 [00:01<00:03, 36.09it/s]
Extracting jepa representations:  26%|██▋       | 45/171 [00:02<00:02, 45.52it/s]
Extracting jepa representations:  32%|███▏      | 54/171 [00:02<00:02, 53.96it/s]
Extracting jepa representations:  37%|███▋      | 63/171 [00:02<00:01, 61.17it/s]
Extracting jepa representations:  42%|████▏     | 72/171 [00:02<00:01, 66.97it/s]
Extracting jepa representations:  47%|████▋     | 81/171 [00:02<00:01, 71.37it/s]
Extracting jepa representations:  53%|█████▎    | 90/171 [00:02<00:01, 74.47it/s]
Extracting jepa representations:  58%|█████▊    | 99/171 [00:02<00:00, 76.90it/s]
Extracting jepa representations:  63%|██████▎   | 108/171 [00:02<00:00, 78.39it/s]
Extracting jepa representations:  68%|██████▊   | 117/171 [00:02<00:00, 79.95it/s]
Extracting jepa representations:  74%|███████▎  | 126/171 [00:02<00:00, 81.11it/s]
Extracting jepa representations:  79%|███████▉  | 135/171 [00:03<00:00, 81.44it/s]
Extracting jepa representations:  84%|████████▍ | 144/171 [00:03<00:00, 82.08it/s]
Extracting jepa representations:  89%|████████▉ | 153/171 [00:03<00:00, 82.40it/s]
Extracting jepa representations:  95%|█████████▍| 162/171 [00:03<00:00, 82.52it/s]
Extracting jepa representations: 100%|██████████| 171/171 [00:03<00:00, 83.13it/s]
Extracting jepa representations: 100%|██████████| 171/171 [00:03<00:00, 48.43it/s]
2025-07-13 21:05:51,052 - INFO - Loading encoder_decoder encoder...
2025-07-13 21:05:51,086 - INFO - Successfully loaded encoder_decoder encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/encoder_decoder/best_encoder.pth

==================================================
Manifold Analysis Results for: JEPA
==================================================
  - Nominal Dimension:     128
  - Participation Ratio:   4.2387
  - 2NN Intrinsic Dim:     0.0944 ± 0.0032
  - Dimension Usage:       3.3%
==================================================


Extracting encoder_decoder representations:   0%|          | 0/171 [00:00<?, ?it/s]
Extracting encoder_decoder representations:   5%|▌         | 9/171 [00:00<00:01, 81.48it/s]
Extracting encoder_decoder representations:  11%|█         | 18/171 [00:00<00:01, 82.49it/s]
Extracting encoder_decoder representations:  16%|█▌        | 27/171 [00:00<00:01, 82.80it/s]
Extracting encoder_decoder representations:  21%|██        | 36/171 [00:00<00:01, 82.99it/s]
Extracting encoder_decoder representations:  26%|██▋       | 45/171 [00:00<00:01, 83.15it/s]
Extracting encoder_decoder representations:  32%|███▏      | 54/171 [00:00<00:01, 83.15it/s]
Extracting encoder_decoder representations:  37%|███▋      | 63/171 [00:00<00:01, 83.09it/s]
Extracting encoder_decoder representations:  42%|████▏     | 72/171 [00:00<00:01, 83.18it/s]
Extracting encoder_decoder representations:  47%|████▋     | 81/171 [00:00<00:01, 83.18it/s]
Extracting encoder_decoder representations:  53%|█████▎    | 90/171 [00:01<00:00, 83.15it/s]
Extracting encoder_decoder representations:  58%|█████▊    | 99/171 [00:01<00:00, 83.22it/s]
Extracting encoder_decoder representations:  63%|██████▎   | 108/171 [00:01<00:00, 83.17it/s]
Extracting encoder_decoder representations:  68%|██████▊   | 117/171 [00:01<00:00, 83.23it/s]
Extracting encoder_decoder representations:  74%|███████▎  | 126/171 [00:01<00:00, 83.24it/s]
Extracting encoder_decoder representations:  79%|███████▉  | 135/171 [00:01<00:00, 83.07it/s]
Extracting encoder_decoder representations:  84%|████████▍ | 144/171 [00:01<00:00, 82.87it/s]
Extracting encoder_decoder representations:  89%|████████▉ | 153/171 [00:01<00:00, 83.01it/s]
Extracting encoder_decoder representations:  95%|█████████▍| 162/171 [00:01<00:00, 83.03it/s]
Extracting encoder_decoder representations: 100%|██████████| 171/171 [00:02<00:00, 84.26it/s]
Extracting encoder_decoder representations: 100%|██████████| 171/171 [00:02<00:00, 83.25it/s]
/home/3157425/rl-worlds/simple-rl-worlds/src/scripts/representation_metrics/analyse_manifold_dimension.py:365: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout(rect=[0, 0, 1, 0.96])
2025-07-13 21:05:55,745 - INFO - ✅ manifold_dimension analysis completed successfully in 23.96s
2025-07-13 21:05:55,745 - INFO - 📊 Measure representation smoothness by correlating pixel and latent distances
2025-07-13 21:05:55,746 - INFO - Starting smoothness analysis...
2025-07-13 21:06:09,526 - INFO - ✅ smoothness analysis completed successfully in 13.78s
2025-07-13 21:06:09,526 - INFO - 📊 Test representation stability under Gaussian noise perturbations
2025-07-13 21:06:09,526 - INFO - Starting robustness analysis...
2025-07-13 21:05:54,682 - INFO - Analysis plots saved to /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/manifold_dimension/manifold_dimension_analysis.png

==================================================
Manifold Analysis Results for: ENCODER_DECODER
==================================================
  - Nominal Dimension:     128
  - Participation Ratio:   2.8763
  - 2NN Intrinsic Dim:     0.0904 ± 0.0023
  - Dimension Usage:       2.2%
==================================================

Analysis complete. View results at: /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/manifold_dimension/manifold_dimension_analysis.png
2025-07-13 21:05:59,611 - INFO - Loading validation data...
2025-07-13 21:05:59,852 - INFO - Loading jepa encoder...
2025-07-13 21:06:00,998 - INFO - Successfully loaded jepa encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/jepa/best_encoder.pth
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples

Analyzing jepa model:   0%|          | 0/171 [00:00<?, ?it/s]
Analyzing jepa model:   1%|          | 1/171 [00:01<04:08,  1.46s/it]
Analyzing jepa model:   5%|▍         | 8/171 [00:01<00:23,  6.81it/s]
Analyzing jepa model:   9%|▉         | 15/171 [00:01<00:11, 13.88it/s]
Analyzing jepa model:  13%|█▎        | 22/171 [00:01<00:06, 21.56it/s]
Analyzing jepa model:  17%|█▋        | 29/171 [00:01<00:04, 29.47it/s]
Analyzing jepa model:  21%|██        | 36/171 [00:01<00:03, 36.79it/s]
Analyzing jepa model:  25%|██▌       | 43/171 [00:02<00:02, 43.64it/s]
Analyzing jepa model:  29%|██▉       | 50/171 [00:02<00:02, 49.53it/s]
Analyzing jepa model:  33%|███▎      | 57/171 [00:02<00:02, 54.46it/s]
Analyzing jepa model:  37%|███▋      | 64/171 [00:02<00:01, 58.28it/s]
Analyzing jepa model:  42%|████▏     | 71/171 [00:02<00:01, 60.79it/s]
Analyzing jepa model:  46%|████▌     | 78/171 [00:02<00:01, 62.87it/s]
Analyzing jepa model:  50%|████▉     | 85/171 [00:02<00:01, 64.42it/s]
Analyzing jepa model:  54%|█████▍    | 92/171 [00:02<00:01, 65.64it/s]
Analyzing jepa model:  58%|█████▊    | 99/171 [00:02<00:01, 66.26it/s]
Analyzing jepa model:  62%|██████▏   | 106/171 [00:03<00:00, 66.86it/s]
Analyzing jepa model:  66%|██████▌   | 113/171 [00:03<00:00, 67.66it/s]
Analyzing jepa model:  70%|███████   | 120/171 [00:03<00:00, 68.08it/s]
Analyzing jepa model:  74%|███████▍  | 127/171 [00:03<00:00, 68.19it/s]
Analyzing jepa model:  78%|███████▊  | 134/171 [00:03<00:00, 68.53it/s]
Analyzing jepa model:  82%|████████▏ | 141/171 [00:03<00:00, 68.64it/s]
Analyzing jepa model:  87%|████████▋ | 148/171 [00:03<00:00, 68.76it/s]
Analyzing jepa model:  91%|█████████ | 155/171 [00:03<00:00, 68.65it/s]
Analyzing jepa model:  95%|█████████▍| 162/171 [00:03<00:00, 68.60it/s]
Analyzing jepa model:  99%|█████████▉| 169/171 [00:03<00:00, 68.52it/s]
Analyzing jepa model: 100%|██████████| 171/171 [00:03<00:00, 43.35it/s]
2025-07-13 21:06:04,958 - INFO - Loading encoder_decoder encoder...
2025-07-13 21:06:04,992 - INFO - Successfully loaded encoder_decoder encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/encoder_decoder/best_encoder.pth

==================================================
Analysis Results for: JEPA
==================================================
  - Regression Slope: 0.0007
  - R-squared (R²):   0.5086
  - Pearson Corr (r): 0.7132
==================================================


Analyzing encoder_decoder model:   0%|          | 0/171 [00:00<?, ?it/s]
Analyzing encoder_decoder model:   4%|▍         | 7/171 [00:00<00:02, 66.10it/s]
Analyzing encoder_decoder model:   8%|▊         | 14/171 [00:00<00:02, 66.18it/s]
Analyzing encoder_decoder model:  12%|█▏        | 21/171 [00:00<00:02, 66.27it/s]
Analyzing encoder_decoder model:  16%|█▋        | 28/171 [00:00<00:02, 66.47it/s]
Analyzing encoder_decoder model:  20%|██        | 35/171 [00:00<00:02, 66.50it/s]
Analyzing encoder_decoder model:  25%|██▍       | 42/171 [00:00<00:01, 66.85it/s]
Analyzing encoder_decoder model:  29%|██▊       | 49/171 [00:00<00:01, 67.10it/s]
Analyzing encoder_decoder model:  33%|███▎      | 56/171 [00:00<00:01, 67.38it/s]
Analyzing encoder_decoder model:  37%|███▋      | 63/171 [00:00<00:01, 67.58it/s]
Analyzing encoder_decoder model:  41%|████      | 70/171 [00:01<00:01, 67.50it/s]
Analyzing encoder_decoder model:  45%|████▌     | 77/171 [00:01<00:01, 67.03it/s]
Analyzing encoder_decoder model:  49%|████▉     | 84/171 [00:01<00:01, 67.02it/s]
Analyzing encoder_decoder model:  53%|█████▎    | 91/171 [00:01<00:01, 66.76it/s]
Analyzing encoder_decoder model:  57%|█████▋    | 98/171 [00:01<00:01, 67.06it/s]
Analyzing encoder_decoder model:  61%|██████▏   | 105/171 [00:01<00:00, 66.92it/s]
Analyzing encoder_decoder model:  65%|██████▌   | 112/171 [00:01<00:00, 67.00it/s]
Analyzing encoder_decoder model:  70%|██████▉   | 119/171 [00:01<00:00, 67.27it/s]
Analyzing encoder_decoder model:  74%|███████▎  | 126/171 [00:01<00:00, 67.28it/s]
Analyzing encoder_decoder model:  78%|███████▊  | 133/171 [00:01<00:00, 66.93it/s]
Analyzing encoder_decoder model:  82%|████████▏ | 140/171 [00:02<00:00, 66.77it/s]
Analyzing encoder_decoder model:  86%|████████▌ | 147/171 [00:02<00:00, 66.75it/s]
Analyzing encoder_decoder model:  90%|█████████ | 154/171 [00:02<00:00, 66.85it/s]
Analyzing encoder_decoder model:  94%|█████████▍| 161/171 [00:02<00:00, 66.70it/s]
Analyzing encoder_decoder model:  98%|█████████▊| 168/171 [00:02<00:00, 66.75it/s]
Analyzing encoder_decoder model: 100%|██████████| 171/171 [00:02<00:00, 67.09it/s]
2025-07-13 21:06:07,580 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-13 21:06:07,593 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
/home/3157425/rl-worlds/simple-rl-worlds/src/scripts/representation_metrics/analyse_smoothness.py:182: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.
  ax2.set_xticklabels(labels)
2025-07-13 21:06:08,481 - INFO - Analysis plots saved to /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/smoothness_analysis/smoothness_comparison.png

==================================================
Analysis Results for: ENCODER_DECODER
==================================================
  - Regression Slope: 0.0012
  - R-squared (R²):   0.2461
  - Pearson Corr (r): 0.4961
==================================================

Analysis complete. View results at: /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/smoothness_analysis/smoothness_comparison.png
2025-07-13 21:06:13,755 - INFO - Loading validation data...
2025-07-13 21:06:14,000 - INFO - Loading jepa encoder...
2025-07-13 21:06:15,158 - INFO - Successfully loaded jepa encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/jepa/best_encoder.pth
Loading dataset from datasets/assault_rep_4.pth...
Successfully loaded PPO-collected dataset for 'ALE/Assault-v5'.
Data loaded: 44277 training samples, 10913 validation samples

Analyzing jepa model:   0%|          | 0/171 [00:00<?, ?it/s]
Analyzing jepa model:   1%|          | 1/171 [00:01<04:19,  1.53s/it]
Analyzing jepa model:   2%|▏         | 3/171 [00:01<01:13,  2.27it/s]
Analyzing jepa model:   3%|▎         | 5/171 [00:01<00:40,  4.11it/s]
Analyzing jepa model:   4%|▍         | 7/171 [00:01<00:26,  6.07it/s]
Analyzing jepa model:   5%|▌         | 9/171 [00:01<00:20,  8.05it/s]
Analyzing jepa model:   6%|▋         | 11/171 [00:02<00:16,  9.92it/s]
Analyzing jepa model:   8%|▊         | 13/171 [00:02<00:13, 11.57it/s]
Analyzing jepa model:   9%|▉         | 15/171 [00:02<00:12, 12.95it/s]
Analyzing jepa model:  10%|▉         | 17/171 [00:02<00:10, 14.09it/s]
Analyzing jepa model:  11%|█         | 19/171 [00:02<00:10, 14.98it/s]
Analyzing jepa model:  12%|█▏        | 21/171 [00:02<00:09, 15.67it/s]
Analyzing jepa model:  13%|█▎        | 23/171 [00:02<00:09, 16.16it/s]
Analyzing jepa model:  15%|█▍        | 25/171 [00:02<00:08, 16.54it/s]
Analyzing jepa model:  16%|█▌        | 27/171 [00:03<00:08, 16.76it/s]
Analyzing jepa model:  17%|█▋        | 29/171 [00:03<00:08, 16.89it/s]
Analyzing jepa model:  18%|█▊        | 31/171 [00:03<00:08, 17.06it/s]
Analyzing jepa model:  19%|█▉        | 33/171 [00:03<00:08, 17.13it/s]
Analyzing jepa model:  20%|██        | 35/171 [00:03<00:07, 17.18it/s]
Analyzing jepa model:  22%|██▏       | 37/171 [00:03<00:07, 17.27it/s]
Analyzing jepa model:  23%|██▎       | 39/171 [00:03<00:07, 17.31it/s]
Analyzing jepa model:  24%|██▍       | 41/171 [00:03<00:07, 17.31it/s]
Analyzing jepa model:  25%|██▌       | 43/171 [00:03<00:07, 17.33it/s]
Analyzing jepa model:  26%|██▋       | 45/171 [00:04<00:07, 17.35it/s]
Analyzing jepa model:  27%|██▋       | 47/171 [00:04<00:07, 17.33it/s]
Analyzing jepa model:  29%|██▊       | 49/171 [00:04<00:07, 17.36it/s]
Analyzing jepa model:  30%|██▉       | 51/171 [00:04<00:06, 17.35it/s]
Analyzing jepa model:  31%|███       | 53/171 [00:04<00:06, 17.36it/s]
Analyzing jepa model:  32%|███▏      | 55/171 [00:04<00:06, 17.30it/s]
Analyzing jepa model:  33%|███▎      | 57/171 [00:04<00:06, 17.34it/s]
Analyzing jepa model:  35%|███▍      | 59/171 [00:04<00:06, 17.32it/s]
Analyzing jepa model:  36%|███▌      | 61/171 [00:04<00:06, 17.34it/s]
Analyzing jepa model:  37%|███▋      | 63/171 [00:05<00:06, 17.35it/s]
Analyzing jepa model:  38%|███▊      | 65/171 [00:05<00:06, 17.38it/s]
Analyzing jepa model:  39%|███▉      | 67/171 [00:05<00:05, 17.40it/s]
Analyzing jepa model:  40%|████      | 69/171 [00:05<00:05, 17.36it/s]
Analyzing jepa model:  42%|████▏     | 71/171 [00:05<00:05, 17.33it/s]
Analyzing jepa model:  43%|████▎     | 73/171 [00:05<00:05, 17.31it/s]
Analyzing jepa model:  44%|████▍     | 75/171 [00:05<00:05, 17.35it/s]
Analyzing jepa model:  45%|████▌     | 77/171 [00:05<00:05, 17.31it/s]
Analyzing jepa model:  46%|████▌     | 79/171 [00:06<00:05, 17.30it/s]
Analyzing jepa model:  47%|████▋     | 81/171 [00:06<00:05, 17.34it/s]
Analyzing jepa model:  49%|████▊     | 83/171 [00:06<00:05, 17.31it/s]
Analyzing jepa model:  50%|████▉     | 85/171 [00:06<00:04, 17.31it/s]
Analyzing jepa model:  51%|█████     | 87/171 [00:06<00:04, 17.31it/s]
Analyzing jepa model:  52%|█████▏    | 89/171 [00:06<00:04, 17.34it/s]
Analyzing jepa model:  53%|█████▎    | 91/171 [00:06<00:04, 17.38it/s]
Analyzing jepa model:  54%|█████▍    | 93/171 [00:06<00:04, 17.37it/s]
Analyzing jepa model:  56%|█████▌    | 95/171 [00:06<00:04, 17.38it/s]
Analyzing jepa model:  57%|█████▋    | 97/171 [00:07<00:04, 17.40it/s]
Analyzing jepa model:  58%|█████▊    | 99/171 [00:07<00:04, 17.36it/s]
Analyzing jepa model:  59%|█████▉    | 101/171 [00:07<00:04, 17.38it/s]
Analyzing jepa model:  60%|██████    | 103/171 [00:07<00:03, 17.39it/s]
Analyzing jepa model:  61%|██████▏   | 105/171 [00:07<00:03, 17.40it/s]
Analyzing jepa model:  63%|██████▎   | 107/171 [00:07<00:03, 17.41it/s]
Analyzing jepa model:  64%|██████▎   | 109/171 [00:07<00:03, 17.40it/s]
Analyzing jepa model:  65%|██████▍   | 111/171 [00:07<00:03, 17.41it/s]
Analyzing jepa model:  66%|██████▌   | 113/171 [00:07<00:03, 17.41it/s]
Analyzing jepa model:  67%|██████▋   | 115/171 [00:08<00:03, 17.35it/s]
Analyzing jepa model:  68%|██████▊   | 117/171 [00:08<00:03, 17.34it/s]
Analyzing jepa model:  70%|██████▉   | 119/171 [00:08<00:03, 17.30it/s]
Analyzing jepa model:  71%|███████   | 121/171 [00:08<00:02, 17.24it/s]
Analyzing jepa model:  72%|███████▏  | 123/171 [00:08<00:02, 17.26it/s]
Analyzing jepa model:  73%|███████▎  | 125/171 [00:08<00:02, 17.31it/s]
Analyzing jepa model:  74%|███████▍  | 127/171 [00:08<00:02, 17.35it/s]
Analyzing jepa model:  75%|███████▌  | 129/171 [00:08<00:02, 17.37it/s]
Analyzing jepa model:  77%|███████▋  | 131/171 [00:09<00:02, 17.33it/s]
Analyzing jepa model:  78%|███████▊  | 133/171 [00:09<00:02, 17.32it/s]
Analyzing jepa model:  79%|███████▉  | 135/171 [00:09<00:02, 17.35it/s]
Analyzing jepa model:  80%|████████  | 137/171 [00:09<00:01, 17.31it/s]
Analyzing jepa model:  81%|████████▏ | 139/171 [00:09<00:01, 17.30it/s]
Analyzing jepa model:  82%|████████▏ | 141/171 [00:09<00:01, 17.34it/s]
Analyzing jepa model:  84%|████████▎ | 143/171 [00:09<00:01, 17.35it/s]
Analyzing jepa model:  85%|████████▍ | 145/171 [00:09<00:01, 17.33it/s]
Analyzing jepa model:  86%|████████▌ | 147/171 [00:09<00:01, 17.37it/s]
Analyzing jepa model:  87%|████████▋ | 149/171 [00:10<00:01, 17.39it/s]
Analyzing jepa model:  88%|████████▊ | 151/171 [00:10<00:01, 17.41it/s]
Analyzing jepa model:  89%|████████▉ | 153/171 [00:10<00:01, 17.37it/s]
Analyzing jepa model:  91%|█████████ | 155/171 [00:10<00:00, 17.39it/s]
Analyzing jepa model:  92%|█████████▏| 157/171 [00:10<00:00, 17.41it/s]
Analyzing jepa model:  93%|█████████▎| 159/171 [00:10<00:00, 17.41it/s]
Analyzing jepa model:  94%|█████████▍| 161/171 [00:10<00:00, 17.36it/s]
Analyzing jepa model:  95%|█████████▌| 163/171 [00:10<00:00, 17.33it/s]
Analyzing jepa model:  96%|█████████▋| 165/171 [00:10<00:00, 17.36it/s]
Analyzing jepa model:  98%|█████████▊| 167/171 [00:11<00:00, 17.38it/s]
Analyzing jepa model:  99%|█████████▉| 169/171 [00:11<00:00, 17.37it/s]
Analyzing jepa model: 100%|██████████| 171/171 [00:11<00:00, 15.13it/s]
2025-07-13 21:06:26,462 - INFO - Loading encoder_decoder encoder...
2025-07-13 21:06:26,497 - INFO - Successfully loaded encoder_decoder encoder from /home/3157425/rl-worlds/simple-rl-worlds/weights/encoder_decoder/best_encoder.pth

Analyzing encoder_decoder model:   0%|          | 0/171 [00:00<?, ?it/s]
Analyzing encoder_decoder model:   1%|          | 2/171 [00:00<00:09, 17.29it/s]
Analyzing encoder_decoder model:   2%|▏         | 4/171 [00:00<00:09, 17.30it/s]
Analyzing encoder_decoder model:   4%|▎         | 6/171 [00:00<00:09, 17.34it/s]
Analyzing encoder_decoder model:   5%|▍         | 8/171 [00:00<00:09, 17.26it/s]
Analyzing encoder_decoder model:   6%|▌         | 10/171 [00:00<00:09, 17.26it/s]
Analyzing encoder_decoder model:   7%|▋         | 12/171 [00:00<00:09, 17.33it/s]
Analyzing encoder_decoder model:   8%|▊         | 14/171 [00:00<00:09, 17.38it/s]
Analyzing encoder_decoder model:   9%|▉         | 16/171 [00:00<00:08, 17.41it/s]
Analyzing encoder_decoder model:  11%|█         | 18/171 [00:01<00:08, 17.43it/s]
Analyzing encoder_decoder model:  12%|█▏        | 20/171 [00:01<00:08, 17.45it/s]
Analyzing encoder_decoder model:  13%|█▎        | 22/171 [00:01<00:08, 17.46it/s]
Analyzing encoder_decoder model:  14%|█▍        | 24/171 [00:01<00:08, 17.45it/s]
Analyzing encoder_decoder model:  15%|█▌        | 26/171 [00:01<00:08, 17.46it/s]
Analyzing encoder_decoder model:  16%|█▋        | 28/171 [00:01<00:08, 17.46it/s]
Analyzing encoder_decoder model:  18%|█▊        | 30/171 [00:01<00:08, 17.48it/s]
Analyzing encoder_decoder model:  19%|█▊        | 32/171 [00:01<00:07, 17.48it/s]
Analyzing encoder_decoder model:  20%|█▉        | 34/171 [00:01<00:07, 17.47it/s]
Analyzing encoder_decoder model:  21%|██        | 36/171 [00:02<00:07, 17.49it/s]
Analyzing encoder_decoder model:  22%|██▏       | 38/171 [00:02<00:07, 17.48it/s]
Analyzing encoder_decoder model:  23%|██▎       | 40/171 [00:02<00:07, 17.45it/s]
Analyzing encoder_decoder model:  25%|██▍       | 42/171 [00:02<00:07, 17.39it/s]
Analyzing encoder_decoder model:  26%|██▌       | 44/171 [00:02<00:07, 17.41it/s]
Analyzing encoder_decoder model:  27%|██▋       | 46/171 [00:02<00:07, 17.44it/s]
Analyzing encoder_decoder model:  28%|██▊       | 48/171 [00:02<00:07, 17.45it/s]
Analyzing encoder_decoder model:  29%|██▉       | 50/171 [00:02<00:06, 17.42it/s]
Analyzing encoder_decoder model:  30%|███       | 52/171 [00:02<00:06, 17.37it/s]
Analyzing encoder_decoder model:  32%|███▏      | 54/171 [00:03<00:06, 17.39it/s]
Analyzing encoder_decoder model:  33%|███▎      | 56/171 [00:03<00:06, 17.43it/s]
Analyzing encoder_decoder model:  34%|███▍      | 58/171 [00:03<00:06, 17.43it/s]
Analyzing encoder_decoder model:  35%|███▌      | 60/171 [00:03<00:06, 17.45it/s]
Analyzing encoder_decoder model:  36%|███▋      | 62/171 [00:03<00:06, 17.47it/s]
Analyzing encoder_decoder model:  37%|███▋      | 64/171 [00:03<00:06, 17.46it/s]
Analyzing encoder_decoder model:  39%|███▊      | 66/171 [00:03<00:06, 17.47it/s]
Analyzing encoder_decoder model:  40%|███▉      | 68/171 [00:03<00:05, 17.47it/s]
Analyzing encoder_decoder model:  41%|████      | 70/171 [00:04<00:05, 17.47it/s]
Analyzing encoder_decoder model:  42%|████▏     | 72/171 [00:04<00:05, 17.44it/s]
Analyzing encoder_decoder model:  43%|████▎     | 74/171 [00:04<00:05, 17.42it/s]
Analyzing encoder_decoder model:  44%|████▍     | 76/171 [00:04<00:05, 17.43it/s]
Analyzing encoder_decoder model:  46%|████▌     | 78/171 [00:04<00:05, 17.34it/s]
Analyzing encoder_decoder model:  47%|████▋     | 80/171 [00:04<00:05, 17.32it/s]
Analyzing encoder_decoder model:  48%|████▊     | 82/171 [00:04<00:05, 17.37it/s]
Analyzing encoder_decoder model:  49%|████▉     | 84/171 [00:04<00:05, 17.39it/s]
Analyzing encoder_decoder model:  50%|█████     | 86/171 [00:04<00:04, 17.43it/s]
Analyzing encoder_decoder model:  51%|█████▏    | 88/171 [00:05<00:04, 17.44it/s]
Analyzing encoder_decoder model:  53%|█████▎    | 90/171 [00:05<00:04, 17.41it/s]
Analyzing encoder_decoder model:  54%|█████▍    | 92/171 [00:05<00:04, 17.42it/s]
Analyzing encoder_decoder model:  55%|█████▍    | 94/171 [00:05<00:04, 17.37it/s]
Analyzing encoder_decoder model:  56%|█████▌    | 96/171 [00:05<00:04, 17.41it/s]
Analyzing encoder_decoder model:  57%|█████▋    | 98/171 [00:05<00:04, 17.43it/s]
Analyzing encoder_decoder model:  58%|█████▊    | 100/171 [00:05<00:04, 17.44it/s]
Analyzing encoder_decoder model:  60%|█████▉    | 102/171 [00:05<00:03, 17.45it/s]
Analyzing encoder_decoder model:  61%|██████    | 104/171 [00:05<00:03, 17.45it/s]
Analyzing encoder_decoder model:  62%|██████▏   | 106/171 [00:06<00:03, 17.47it/s]
Analyzing encoder_decoder model:  63%|██████▎   | 108/171 [00:06<00:03, 17.43it/s]
Analyzing encoder_decoder model:  64%|██████▍   | 110/171 [00:06<00:03, 17.43it/s]
Analyzing encoder_decoder model:  65%|██████▌   | 112/171 [00:06<00:03, 17.43it/s]
Analyzing encoder_decoder model:  67%|██████▋   | 114/171 [00:06<00:03, 17.38it/s]
Analyzing encoder_decoder model:  68%|██████▊   | 116/171 [00:06<00:03, 17.41it/s]
Analyzing encoder_decoder model:  69%|██████▉   | 118/171 [00:06<00:03, 17.43it/s]
Analyzing encoder_decoder model:  70%|███████   | 120/171 [00:06<00:02, 17.44it/s]
Analyzing encoder_decoder model:  71%|███████▏  | 122/171 [00:07<00:02, 17.46it/s]
Analyzing encoder_decoder model:  73%|███████▎  | 124/171 [00:07<00:02, 17.46it/s]
Analyzing encoder_decoder model:  74%|███████▎  | 126/171 [00:07<00:02, 17.47it/s]
Analyzing encoder_decoder model:  75%|███████▍  | 128/171 [00:07<00:02, 17.48it/s]
Analyzing encoder_decoder model:  76%|███████▌  | 130/171 [00:07<00:02, 17.48it/s]
Analyzing encoder_decoder model:  77%|███████▋  | 132/171 [00:07<00:02, 17.44it/s]
Analyzing encoder_decoder model:  78%|███████▊  | 134/171 [00:07<00:02, 17.38it/s]
Analyzing encoder_decoder model:  80%|███████▉  | 136/171 [00:07<00:02, 17.41it/s]
Analyzing encoder_decoder model:  81%|████████  | 138/171 [00:07<00:01, 17.44it/s]
Analyzing encoder_decoder model:  82%|████████▏ | 140/171 [00:08<00:01, 17.45it/s]
Analyzing encoder_decoder model:  83%|████████▎ | 142/171 [00:08<00:01, 17.47it/s]
Analyzing encoder_decoder model:  84%|████████▍ | 144/171 [00:08<00:01, 17.46it/s]
Analyzing encoder_decoder model:  85%|████████▌ | 146/171 [00:08<00:01, 17.48it/s]
Analyzing encoder_decoder model:  87%|████████▋ | 148/171 [00:08<00:01, 17.48it/s]
Analyzing encoder_decoder model:  88%|████████▊ | 150/171 [00:08<00:01, 17.48it/s]
Analyzing encoder_decoder model:  89%|████████▉ | 152/171 [00:08<00:01, 17.49it/s]
Analyzing encoder_decoder model:  90%|█████████ | 154/171 [00:08<00:00, 17.48it/s]
Analyzing encoder_decoder model:  91%|█████████ | 156/171 [00:08<00:00, 17.43it/s]
Analyzing encoder_decoder model:  92%|█████████▏| 158/171 [00:09<00:00, 17.32it/s]
Analyzing encoder_decoder model:  94%|█████████▎| 160/171 [00:09<00:00, 17.36it/s]
Analyzing encoder_decoder model:  95%|█████████▍| 162/171 [00:09<00:00, 17.40it/s]
Analyzing encoder_decoder model:  96%|█████████▌| 164/171 [00:09<00:00, 17.42it/s]
Analyzing encoder_decoder model:  97%|█████████▋| 166/171 [00:09<00:00, 17.39it/s]
Analyzing encoder_decoder model:  98%|█████████▊| 168/171 [00:09<00:00, 17.41it/s]
Analyzing encoder_decoder model:  99%|█████████▉| 170/171 [00:09<00:00, 17.36it/s]
Analyzing encoder_decoder model: 100%|██████████| 171/171 [00:09<00:00, 17.47it/s]
2025-07-13 21:06:36,329 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-13 21:06:36,340 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
/home/3157425/rl-worlds/simple-rl-worlds/src/scripts/representation_metrics/analyse_robustness.py:143: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.
  ax2.set_xticklabels(labels)
2025-07-13 21:06:36,992 - INFO - Analysis plots saved to /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/robustness_analysis/robustness_comparison.png

============================================================
                Robustness Analysis Summary                 
   Mean Latent Distance E||φ(s̃) - φ(s)|| (± Std. Error)    
2025-07-13 21:06:38,028 - INFO - ✅ robustness analysis completed successfully in 28.50s
2025-07-13 21:06:38,028 - INFO - 
============================================================
2025-07-13 21:06:38,028 - INFO - 🏁 REPRESENTATION METRICS ANALYSIS SUMMARY
2025-07-13 21:06:38,028 - INFO - ============================================================
2025-07-13 21:06:38,028 - INFO - ⏱️  Total duration: 1817.88s
2025-07-13 21:06:38,028 - INFO - ✅ Successful analyses (4): neighborhood_preservation, manifold_dimension, smoothness, robustness
2025-07-13 21:06:38,028 - INFO - 🎉 All analyses completed successfully!
2025-07-13 21:06:38,028 - INFO - 
📁 Check the evaluation_plots/ directory for generated visualizations
2025-07-13 21:06:38,028 - INFO - 📊 Generated plots can be found in:
2025-07-13 21:06:38,028 - INFO -    • evaluation_plots/neighborhood_preservation/
2025-07-13 21:06:38,028 - INFO -    • evaluation_plots/manifold_dimension/
2025-07-13 21:06:38,028 - INFO -    • evaluation_plots/smoothness_analysis/
2025-07-13 21:06:38,028 - INFO -    • evaluation_plots/robustness_analysis/
============================================================
Noise Level    Model               Mean Distance            
------------------------------------------------------------
0.01           JEPA                4.3440 ± 0.0066
0.01           ENCODER_DECODER     9.5369 ± 0.0267
------------------------------------------------------------
0.05           JEPA                4.2337 ± 0.0067
0.05           ENCODER_DECODER     9.2440 ± 0.0256
------------------------------------------------------------
0.10           JEPA                4.0812 ± 0.0069
0.10           ENCODER_DECODER     8.9502 ± 0.0243
------------------------------------------------------------
0.15           JEPA                3.8647 ± 0.0071
0.15           ENCODER_DECODER     8.7409 ± 0.0233
------------------------------------------------------------
0.20           JEPA                3.6306 ± 0.0073
0.20           ENCODER_DECODER     8.6659 ± 0.0232
------------------------------------------------------------

Analysis complete. View results at: /home/3157425/rl-worlds/simple-rl-worlds/evaluation_plots/robustness_analysis/robustness_comparison.png
